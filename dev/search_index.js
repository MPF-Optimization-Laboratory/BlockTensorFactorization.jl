var documenterSearchIndex = {"docs":
[{"location":"deprecated/matrixtensorfactor/#Matrix-Tensor-Factor","page":"Matrix Tensor Factor","title":"Matrix Tensor Factor","text":"","category":"section"},{"location":"deprecated/matrixtensorfactor/","page":"Matrix Tensor Factor","title":"Matrix Tensor Factor","text":"Old interface from MatrixTensorFactor, with the improved backend from factorize.","category":"page"},{"location":"deprecated/matrixtensorfactor/","page":"Matrix Tensor Factor","title":"Matrix Tensor Factor","text":"warning: Warning\nThis submodule is deprecated and may be removed in future versions. It is preferred to use factorize rather than nnmtf.","category":"page"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor","text":"Matrix-Tensor Factorization\n\n\n\n\n\n","category":"module"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_CRITERIA","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_CRITERIA","text":"IMPLIMENTED_CRITERIA::Set{Symbol}\n\n:ncone: vector-set distance between the -gradient of the objective and the normal cone\n:iterates: A,B before and after one iteration are close in L2 norm\n:objective: objective is small\n:relativeerror: relative error is small (when normalize=:nothing) or   mean relative error averaging fibres or slices when the normalization is :fibres or   :slices respectfuly.\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_METRICS","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_METRICS","text":"IMPLIMENTED_METRICS::Set{Symbol}\n\n:L1: the default; ensure sums of entries in each fibre or slice (according to normalize)   are equal to 1\nTODO :L2: ensure the sum of squares of entries are 1\n:Linfty: ensures the maximum entry is 1\n:nothing: do not enforce a metric to equal 1\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_NORMALIZATIONS","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_NORMALIZATIONS","text":"IMPLIMENTED_NORMALIZATIONS::Set{Symbol}\n\n:fibres: set sum_k=1^K Brjk = 1 for all r j, or when projection==:nnscale,   set sum_j=1^Jsum_k=1^K Brjk = J for all r\n:slices: set sum_j=1^Jsum_k=1^K Brjk = 1 for all r\n:nothing: does not enforce any normalization of B\n:rows: set a metric on eachslice(X; dims=1) to 1, which is equivelent to eachrow(X) when X is a matrix\n:cols: set a metric on eachslice(X; dims=2) to 1, which is equivelent to eachcol(X) when X is a matrix\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_OPTIONS","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_OPTIONS","text":"Lists all implimented options\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_PROJECTIONS","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_PROJECTIONS","text":"IMPLIMENTED_PROJECTIONS::Set{Symbol}\n\n:nnscale: Two stage block coordinate decent; 1) projected gradient decent onto nonnegative   orthant, 2) shift any weight from B to A according to normalization. Equivilent to   :nonnegative when normalization==:nothing.\n:simplex: Euclidian projection onto the simplex blocks accoring to normalization\n:nonnegative: zero out negative entries\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_STEPSIZES","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.IMPLIMENTED_STEPSIZES","text":"IMPLIMENTED_STEPSIZES::Set{Symbol}\n\n:lipschitz: gradient step 1/L for lipschitz constant L\n:spg: spectral projected gradient stepsize\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor._avg_fibre_normalize!-Tuple{AbstractMatrix, AbstractArray}","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor._avg_fibre_normalize!","text":"Rescales A and B so each factor (3 fibres) of B has similar magnitude.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor._init-Tuple","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor._init","text":"Default initialization\n\n\n\n\n\n","category":"method"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor._slice_normalize!-Tuple{AbstractMatrix, AbstractArray}","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor._slice_normalize!","text":"Rescales A and B so each factor (horizontal slices) of B has similar magnitude.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.nnmtf","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.nnmtf","text":"nnmtf(Y::AbstractArray, R::Integer; kwargs...)\n\nNon-negatively matrix-tensor factorizes an order N tensor Y with a given \"rank\" R.\n\nFor an order N=3 tensor, this factorizes Y approx A B where displaystyle Yijk approx sum_r=1^R Air*Brjk and the factors A B geq 0 are nonnegative.\n\nFor higher orders, this becomes displaystyle Yi1i2iN approx sum_r=1^R Ai1r*Bri2iN\n\nNote there may NOT be a unique optimal solution\n\nArguments\n\nY::AbstractArray{T,N}: tensor to factorize\nR::Integer: rank to factorize Y (size(A, 2) and size(B, 1))\n\nKeywords\n\nmaxiter::Integer=100: maxmimum number of iterations\ntol::Real=1e-3: desiered tolerance for the convergence criterion\nrescale_AB::Bool=true: scale B at each iteration so that the factors (horizontal slices) have similar 3-fiber sums.\nrescale_Y::Bool=true: Preprocesses the input Y to have normalized 3-fiber sums (on average), and rescales the final B so Y=A*B.\nnormalize::Symbol=:fibres: part of B that should be normalized (must be in IMPLIMENTED_NORMALIZATIONS)\nnormalizeA::Symbol=:rows: part of A that should be normalized (must be in IMPLIMENTED_NORMALIZATIONS)\nprojection::Symbol=:nnscale: constraint to use and method for enforcing it (must be in IMPLIMENTED_PROJECTIONS)\nmetric::Symbol=:L1: under what metric the fibres/slices are normalized (must be in IMPLIMENTED_METRICS)\ncriterion::Symbol=:ncone: how to determine if the algorithm has converged (must be in IMPLIMENTED_CRITERIA)\nstepsize::Symbol=:lipschitz: used for the gradient decent step (must be in IMPLIMENTED_STEPSIZES)\nmomentum::Bool=false: use momentum updates\ndelta::Real=0.9999: safeguard for maximum amount of momentum (see eq (3.5) Xu & Yin 2013)\nR_max::Integer=size(Y, 1): maximum rank to try if R is not given\nprojectionA::Symbol=projection: projection to use on factor A (must be in IMPLIMENTED_PROJECTIONS)\nprojectionB::Symbol=projection: projection to use on factor B (must be in IMPLIMENTED_PROJECTIONS)\nmetricA::Symbol=metric: the metric to use for factor A (must be in IMPLIMENTED_METRICS)\nmetricB::Symbol=metric: the metric to use for factor B (must be in IMPLIMENTED_METRICS)\nscaleBtoA::Bool=true: when using projection=:nnscale, if the weights should be moved from B to A, or A to B\nA_init::AbstractMatrix=nothing: initial A for the iterative algorithm. Should be kept as nothing if R is not given.\nB_init::AbstractArray=nothing: initial B for the iterative algorithm. Should be kept as nothing if R is not given.\n\nReturns\n\nA::Matrix{Float64}: the matrix A in the factorization Y ≈ A * B\nB::Array{Float64, N}: the tensor B in the factorization Y ≈ A * B\nrel_errors::Vector{Float64}: relative errors at each iteration\nnorm_grad::Vector{Float64}: norm of the full gradient at each iteration\ndist_Ncone::Vector{Float64}: distance of the -gradient to the normal cone at each iteration\nIf R was estimated, also returns the optimal R::Integer\n\nImplimentation of block coordinate decent updates\n\nWe calculate the partial gradients and corresponding Lipschitz constants like so:\n\nbeginalign\n  boldsymbolP^tqr =textstylesum_jk boldsymbolmathscrB^nqjk boldsymbolmathscrB^nrjk\n  boldsymbolQ^tir =textstylesum_jkboldsymbolmathscrYijk boldsymbolmathscrB^nrjk \n  nabla_A f(boldsymbolA^tboldsymbolmathscrB^t) = boldsymbolA^t boldsymbolP^t - boldsymbolQ^t \n  L_A = leftlVert boldsymbolP^t rightrVert_2\nendalign\n\nSimilarly for boldsymbolmathscrB:\n\nbeginalign\n  boldsymbolT^t+1=(boldsymbolA^t+frac12)^top boldsymbolA^t+frac12\n  boldsymbolmathscrU^t+1=(boldsymbolA^t+frac12)^top boldsymbolmathscrY \n  nabla_boldsymbolmathscrB f(boldsymbolA^t+frac12boldsymbolmathscrB^t) =  boldsymbolT^t+1 boldsymbolmathscrB^t - boldsymbolmathscrU^t+1 \n  L_B = leftlVert boldsymbolT^t+1 rightrVert_2\nendalign\n\nTo ensure the iterates stay \"close\" to normalized, we introduce a renormalization step after the projected gradient updates:\n\nbeginalign\n    boldsymbolC rr=frac1Jtextstylesum_jk boldsymbolmathscrB^t+frac12rjk\n    boldsymbolA^t+1= boldsymbolA^t+frac12 boldsymbolC\n    boldsymbolmathscrB^t+1= (boldsymbolC^t+1)^-1boldsymbolmathscrB^t+frac12\nendalign\n\nWe typicaly use the following convergence criterion:\n\nd(-nabla ell(boldsymbolA^tboldsymbolmathscrB^t) N_mathcalC(boldsymbolA^tboldsymbolmathscrB^t))^2leqdelta^2 R(I+JK)\n\n\n\n\n\n","category":"function"},{"location":"deprecated/matrixtensorfactor/#BlockTensorFactorization.MatrixTensorFactor.rescaleAB!-Tuple{Any, Any}","page":"Matrix Tensor Factor","title":"BlockTensorFactorization.MatrixTensorFactor.rescaleAB!","text":"Rescales A and B so each factor (horizontal slices) of B has similar magnitude.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#Density-Estimation-Tools","page":"Density Estimation Tools","title":"Density Estimation Tools","text":"","category":"section"},{"location":"deprecated/densityestimationtools/","page":"Density Estimation Tools","title":"Density Estimation Tools","text":"Helper functions for working with data for density tensors.","category":"page"},{"location":"deprecated/densityestimationtools/","page":"Density Estimation Tools","title":"Density Estimation Tools","text":"warning: Warning\nThis submodule is deprecated and may be removed in future versions.","category":"page"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.DEFAULT_ALPHA","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.DEFAULT_ALPHA","text":"DEFAULT_ALPHA = 0.9::Real\n\nSmoothing parameter for calculating a kernel's bandwidth.\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.DEFAULT_N_SAMPLES","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.DEFAULT_N_SAMPLES","text":"DEFAULT_N_SAMPLES = 64::Int\n\nNumber of samples to use when standardizing a vector of density estimates.\n\n\n\n\n\n","category":"constant"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools._in2drange-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools._in2drange","text":"Returns a function that checks if each coordinate is in the inner P percentile of the values in vs.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools._inrange-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools._inrange","text":"Returns a function that checks if a value is in the inner P percentile of the values in v.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.coordzip-Tuple{Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.coordzip","text":"coordzip(rcoords)\n\nZips the \"x\" and \"y\" values together into a list of x coords and y coords. Example –––- coordzip([(0,0), (1,1), (1,1), (1,1), (1,2), (1,2)])\n\n[[0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 2, 2]]\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.default_bandwidth","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.default_bandwidth","text":"default_bandwidth(data; alpha=0.9, inner_percentile=100)\n\nCoppied from KernelDensity since this function is not exported. I want access to it so that the same bandwidth can be used for different densities for the same measurements.\n\n\n\n\n\n","category":"function"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.filter_2d_inner_percentile-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.filter_2d_inner_percentile","text":"Filters 2d elements so only the ones in the inner P percentile remain. See filter_inner_percentile.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.filter_inner_percentile-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.filter_inner_percentile","text":"Filters elements so only the ones in the inner P percentile remain. See filter_2d_inner_percentile.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.kde2d-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.kde2d","text":"kde2d((xs, ys), values)\n\nPerforms a 2d KDE based on two lists of coordinates, and the value at those coordinates. Input ––-\n\nxs, ys::Vector{Real}: coordinates/locations of samples\nvalues::Vector{Integer}: value of the sample\n\nReturns\n\nf::BivariateKDE use f.x, f.y for the location of the (re)sampled KDE,\n\nand f.density for the sample values of the KDE\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.make_densities-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.make_densities","text":"make_densities(s::Sink; kwargs...)\nmake_densities(s::Sink, domains::AbstractVector{<:AbstractVector}; kwargs...)\n\nEstimates the densities for each measurement in a Sink.\n\nWhen given domains, a list where each entry is a domain for a different measurement, resample the kernel on this domain.\n\nParameters\n\nbandwidths::AbstractVector{<:Real}: list of bandwidths used for each measurement's\n\ndensity estimation\n\ninner_percentile::Integer=100: value between 0 and 100 that filters out each measurement\n\nby using the inner percentile range. This can help remove outliers and focus in on where the bulk of the data is.\n\nReturns\n\ndensity_estimates::Vector{UnivariateKDE}\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.make_densities2d-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.make_densities2d","text":"makedensities2d(s::Sink; kwargs...) makedensities2d(s::Sink, domains::AbstractVector{<:AbstractVector}; kwargs...)\n\nSimilar to make_densities but performs the KDE on 2 measurements jointly.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.repeatcoord-Tuple{Any, Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.repeatcoord","text":"repeatcoord(coordinates, values)\n\nRepeates coordinates the number of times given by values.\n\nBoth lists should be the same length.\n\nExample\n\ncoordinates = [(0,0), (1,1), (1,2)] values = [1, 3, 2] repeatcoord(coordinates, values)\n\n[(0,0), (1,1), (1,1), (1,1), (1,2), (1,2)]\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.standardize_2d_KDEs-Tuple{Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.standardize_2d_KDEs","text":"standardize_2d_KDEs(KDEs::AbstractVector{BivariateKDE}; n_samples=DEFAULT_N_SAMPLES,)\n\nResample the densities so they all are sampled from the same x and y coordinates.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.standardize_KDEs-Tuple{AbstractVector{<:AbstractVector{<:KernelDensity.UnivariateKDE}}}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.standardize_KDEs","text":"Resample the densities within each sink/source so that like-measurements use the same scale.\n\n\n\n\n\n","category":"method"},{"location":"deprecated/densityestimationtools/#BlockTensorFactorization.DensityEstimationTools.standardize_KDEs-Tuple{Any}","page":"Density Estimation Tools","title":"BlockTensorFactorization.DensityEstimationTools.standardize_KDEs","text":"standardize_KDEs(KDEs::AbstractVector{UnivariateKDE}; n_samples=DEFAULT_N_SAMPLES,)\n\nResample the densities so they all are sampled from the same domain.\n\n\n\n\n\n","category":"method"},{"location":"tutorial/blockupdateorder/#Block-Update-Order","page":"Block Update Order","title":"Block Update Order","text":"","category":"section"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"The default order the blocks are updated is cyclically through each factor of the decomposition D::AbstractDecomposition, in the order of factors(D). For AbstractTucker decompositions like Tucker, Tucker-1, and CP, this means starting with the core, followed by the matrix factor for the first dimension, second dimension, and so on.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"As an example, this would be the default order of updates for nonnegative CP decomposition on an order 3 tensor.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"BlockedUpdate(\n    MomentumUpdate(1, lipschitz)\n    GradientStep(1, gradient, LipschitzStep)\n    Projection(1, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(2, lipschitz)\n    GradientStep(2, gradient, LipschitzStep)\n    Projection(2, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(3, lipschitz)\n    GradientStep(3, gradient, LipschitzStep)\n    Projection(3, Entrywise(ReLU, isnonnegative))\n)","category":"page"},{"location":"tutorial/blockupdateorder/#Randomizing-Block-Updates-Order","page":"Block Update Order","title":"Randomizing Block Updates Order","text":"","category":"section"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"The order of updates can be randomized with the random_order keyword.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"X, stats, kwargs = factorize(Y; random_order=true)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"By default, this will keep momentum steps, gradient steps, and constraint steps for each factor together as a block, in this order.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"A possible order of updates could be the following. Note that the updates for each factor are grouped together, but each factor is updated in a random order.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"BlockedUpdate(\n    BlockedUpdate(\n        MomentumUpdate(2, lipschitz)\n        GradientStep(2, gradient, LipschitzStep)\n        Projection(2, Entrywise(ReLU, isnonnegative))\n    )\n    BlockedUpdate(\n        MomentumUpdate(1, lipschitz)\n        GradientStep(1, gradient, LipschitzStep)\n        Projection(1, Entrywise(ReLU, isnonnegative))\n    )\n    BlockedUpdate(\n        MomentumUpdate(3, lipschitz)\n        GradientStep(3, gradient, LipschitzStep)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n    )\n)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"For more randomization, use the recursive_random_order keyword which will also randomize the order in which the momentum steps, gradient steps, and constraint steps are performed.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"X, stats, kwargs = factorize(Y; recursive_random_order=true)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"A possible order of updates could now be the following. The updates for each factor are still grouped together, but the updates within each block appear in a random order.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"BlockedUpdate(\n    BlockedUpdate(\n        Projection(2, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(2, lipschitz)\n        GradientStep(2, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        MomentumUpdate(1, lipschitz)\n        Projection(1, Entrywise(ReLU, isnonnegative))\n        GradientStep(1, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        GradientStep(3, gradient, LipschitzStep)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(3, lipschitz)\n    )\n)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"The opposite of this would be to keep the outer order of blocks as given, but randomize the order which the updates for each factor gets applied, use the following code.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"X, stats, kwargs = factorize(Y; recursive_random_order=true, random_order=false, group_by_factor=true)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"A possible order of updates could now be the following. Note the order of factors is preserved (1, 2, 3) but the inner BlockedUpdates have a random order.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"BlockedUpdate(\n    BlockedUpdate(\n        Projection(1, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(1, lipschitz)\n        GradientStep(1, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        MomentumUpdate(2, lipschitz)\n        Projection(2, Entrywise(ReLU, isnonnegative))\n        GradientStep(2, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        GradientStep(3, gradient, LipschitzStep)\n        MomentumUpdate(3, lipschitz)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n    )\n)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"Note all the previously mentioned options still keeps the various updates for each factor together. For full randomization, use the following code.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"X, stats, kwargs = factorize(Y;\n    recursive_random_order=true, group_by_factor=false)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"A possible order of updates could now be the following. Note that every update can appear anywhere in the order.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"BlockedUpdate(\n    Projection(3, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(2, lipschitz)\n    GradientStep(2, gradient, LipschitzStep)\n    MomentumUpdate(1, lipschitz)\n    GradientStep(1, gradient, LipschitzStep)\n    Projection(2, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(3, lipschitz)\n    MomentumUpdate(2, lipschitz)\n    Projection(1, Entrywise(ReLU, isnonnegative))\n    GradientStep(3, gradient, LipschitzStep)\n)","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"The complete behaviour is summarized in the table below.","category":"page"},{"location":"tutorial/blockupdateorder/","page":"Block Update Order","title":"Block Update Order","text":"group_by_factor random_order recursive_random_order Description\nfalse false false In the order given\nfalse false true In order given, but randomize how existing blocks are ordered (recursively)\nfalse true false Randomize updates, but keep existing blocks in order\nfalse true true Fully random\ntrue false false In the order given\ntrue false true In order of factors, but updates for each factor a random order\ntrue true false Random order of factors, preserve order of updates within each factor\ntrue true true Almost fully random, but updates for each factor are done together","category":"page"},{"location":"tutorial/iterationstats/#Iteration-Stats","page":"Iteration Stats","title":"Iteration Stats","text":"","category":"section"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.AbstractStat-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.AbstractStat","text":"An AbstractStat is a type which, when created, can be applied to the four arguments (X::AbstractDecomposition, Y::AbstractArray, previous::Vector{<:AbstractDecomposition}, parameters::Dict) to (usually) return a number.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/","page":"Iteration Stats","title":"Iteration Stats","text":"The following stats are supported inputs to the stats keyword in factorize.","category":"page"},{"location":"tutorial/iterationstats/","page":"Iteration Stats","title":"Iteration Stats","text":"Iteration\nGradientNorm\nGradientNNCone\nObjectiveValue\nObjectiveRatio\nRelativeError\nIterateNormDiff\nIterateRelativeDiff\nEuclidianStepSize\nEuclidianLipschitz\nFactorNorms","category":"page"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.Iteration-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.Iteration","text":"Iteration <: AbstractStat\n\nIteration number.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.GradientNorm-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.GradientNorm","text":"GradientNorm{T} <: AbstractStat\n\n2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.GradientNNCone-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.GradientNNCone","text":"GradientNNCone{T} <: AbstractStat\n\n2-norm vector-set distance between the negative gradient and nonnegative cone at the iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.ObjectiveValue-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.ObjectiveValue","text":"ObjectiveValue{T<:AbstractObjective} <: AbstractStat\n\nThe current objective value.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.ObjectiveRatio-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.ObjectiveRatio","text":"ObjectiveRatio{T<:AbstractObjective} <: AbstractStat\n\nRatio between the previous and current objective value.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.RelativeError-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.RelativeError","text":"RelativeError{T<:Function} <: AbstractStat\n\nRelative error between the decomposition model, and input array.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.IterateNormDiff-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.IterateNormDiff","text":"IterateNormDiff{T<:Function} <: AbstractStat\n\n2-norm of the difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.IterateRelativeDiff-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.IterateRelativeDiff","text":"IterateRelativeDiff{T<:Function} <: AbstractStat\n\nRelative difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.EuclidianStepSize-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.EuclidianStepSize","text":"The 2-norm of the stepsizes that would be taken for all blocks.\n\nFor example, if there are two blocks, and we would take a stepsize of A to update one block and B to update the other, this would return sqrt(A^2 + B^2).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.EuclidianLipschitz-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.EuclidianLipschitz","text":"The 2-norm of the lipschitz constants that would be taken for all blocks.\n\nNeed the stepsizes to be lipschitz steps since it is calculated similarly to EuclidianStepSize.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.FactorNorms-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.FactorNorms","text":"FactorNorms(; norm, kwargs...)\n\nMakes a tuple containing the norm of each factor in the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/","page":"Iteration Stats","title":"Iteration Stats","text":"The following are subtype of AbstractStat but are for auxiliary features.","category":"page"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.PrintStats-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.PrintStats","text":"PrintStats(; kwargs...)\n\nDoes not use any of the kwargs. Simply prints the most recent row of the stats.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.DisplayDecomposition-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.DisplayDecomposition","text":"DisplayDecomposition(; kwargs...)\n\nDoes not use any of the kwargs. Simply displays the current iteration.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#Constrained-Factorization","page":"Constrained Factorization","title":"Constrained Factorization","text":"","category":"section"},{"location":"tutorial/constraints/#Example","page":"Constrained Factorization","title":"Example","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"A number of constraints are ready-to-use out of the box. Say you would like to perform a rank 5 CP decomposition of a 3rd order tensor, such that every column is constrainted to the simplex. This can be accomplished by the following code.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"X, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5, constraints=simplex_cols!)","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Only one constraint is given when there are 3 factors in the CP decomposition of Y, so it is assumed this constraint applies to every factor.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Say you instead you only want the 1st factor to have columns constrained to the simplex, and the 3rd factor to be nonnegative. You can use the following code.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"X, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5,\n    constraints=[simplex_cols!, noconstraint, nonnegative!])","category":"page"},{"location":"tutorial/constraints/#Ready-to-use-Constraints","page":"Constrained Factorization","title":"Ready-to-use Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#Entrywise","page":"Constrained Factorization","title":"Entrywise","text":"","category":"section"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.Entrywise-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.Entrywise","text":"Entrywise constraint. Note both apply and check needs to be performed entrywise on an array\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"The constraints nonnegative! and binary! ensure each entry in a factor is nonnegative and either 0 or 1 respectively.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.nonnegative!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.nonnegative!","text":"Entrywise(ReLU, isnonnegative)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.binary!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.binary!","text":"Entrywise(binaryproject, x -> x in (0, 1))\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"If you want every entry to be in the closed interval a, b, you can use IntervalConstraint(a, b).","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.IntervalConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.IntervalConstraint","text":"Entrywise(x -> clamp(x, a, b), x -> a ≤ x ≤ b)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#Normalizations","page":"Constrained Factorization","title":"Normalizations","text":"","category":"section"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ProjectedNormalization-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ProjectedNormalization","text":"ProjectedNormalization(projection, norm; whats_normalized=identity)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be the same size as the output of whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"These ensure a factor is normalized according to the L1, L2, & L infinity norms. This is accomplished through a Euclidean projections onto the unit ball.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.l1normalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.l1normalize!","text":"ProjectedNormalization(l1norm, l1project!)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.l2normalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.l2normalize!","text":"ProjectedNormalization(projection, norm; whats_normalized=identity)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be the same size as the output of whats_normalized.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.linftynormalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.linftynormalize!","text":"ProjectedNormalization(linftynorm, linftyproject!)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Each come in versions that constrain each row, column, order-1 slice, & order-(1, 2) slice to the associated unit norm ball.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"l1normalize_rows!\nl1normalize_cols!\nl1normalize_1slices!\nl1normalize_12slices!\n\nl2normalize_rows!\nl2normalize_cols!\nl2normalize_1slices!\nl2normalize_12slices!\n\nlinftynormalize_rows!\nlinftynormalize_cols!\nlinftynormalize_1slices!\nlinftynormalize_12slices!","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"warning: Warning\nProjection onto the unit norm ball from the origin is not unique at the origin so do not expect consistant behaviour with an all-zeros input.","category":"page"},{"location":"tutorial/constraints/#Scaling-Constraints","page":"Constrained Factorization","title":"Scaling Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ScaledNormalization-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ScaledNormalization","text":"ScaledNormalization(norm; whats_normalized=identity, scale=1)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be broadcast-able with the output of whats_normalized. Lastly, scale can be a Function which will act on an AbstractArray{<:Real} and return something that is broadcast-able whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Each previously listed normalization has an associated scaled normalization. These ensure the relevant subarrays are normalized, but rather than enforce these by a Euclidean projection, they simply divide by its norm. This is equivalent to the Euclidean projection onto the L2 norm ball, but is a different operation for the L1 and L infinity balls. This offers the advantage that other factors can be \"rescaled\" to compensate for this division which is not normally possible with the projections onto the L1 and L infinity balls.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"note: Note\nBy default, when these constraints are applied, they will \"rescale\" the other factors to minimize the change in the product of all the factors.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"details: Example\nSay we are performing CPDecomposition on a matrix Y. This is equivalent to factorizing Y = A * B'. If we would like all columns of B (rows of B transpose) to be on the L1 ball, rather than projecting each column, we can instead divide each column of B by its L1 norm, and multiply the associated column of A by this amount. This has the advantage of enforcing our constraint without effecting the product A * B', whereas a projection would possibly change this product.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"l1scale!\nl1scale_rows!\nl1scale_cols!\nl1scale_1slices!\nl1scale_12slices!\n\nl2scale!\nl2scale_rows!\nl2scale_cols!\nl2scale_1slices!\nl2scale_12slices!\n\nlinftyscale!\nlinftyscale_rows!\nlinftyscale_cols!\nlinftyscale_1slices!\nlinftyscale_12slices!","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"There is also a set of constraints that ensure the order-(1,2) slices are scaled on average. This makes preserving a Tucker1 product possible where you would like each order-(1,2) normalized.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"l1scale_average12slices!\nl2scale_average12slices!\nlinftyscale_average12slices!","category":"page"},{"location":"tutorial/constraints/#Simplex-Constraint","page":"Constrained Factorization","title":"Simplex Constraint","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Similar to the L1 normalization constraint, these constraints ensure the relevant subarrays are on the L1 ball. But these also ensure all entries are positive. This is enforced with a single Euclidean projection onto the relevant simplex.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"simplex!\nsimplex_cols!\nsimplex_1slices!\nsimplex_12slices!","category":"page"},{"location":"tutorial/constraints/#Advanced-Constraints","page":"Constrained Factorization","title":"Advanced Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#Ending-with-a-different-set-of-constraints","page":"Constrained Factorization","title":"Ending with a different set of constraints","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"It is possible to apply a different set of constraints at the end of the algorithm than what is enforced during the iterations. For example, you can perform nonnegative Tucker factorization of a tensor Y, but apply a simplex constraint on the core at the very end.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"X, stats, kwargs = factorize(Y; model=Tucker, rank=2,\n    constraints=nonnegative!,\n    final_constraints=[simplex!, noconstraint, noconstraint, noconstraint])","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"In the case where constraints effect other factors (e.g. l2scale!), you may want to perform a final pass of the constraints to ensure each factor is scaled correctly, without rescaling/effecting other factors.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"X, stats, kwargs = factorize(Y; model=CPDecomposition, rank=3,\n    constraints=l2scale_cols!,\n    constrain_output=true)","category":"page"},{"location":"tutorial/constraints/#Composing-Constraints","page":"Constrained Factorization","title":"Composing Constraints","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"AbstractConstraint types can be composed with \\circ (and hitting tab to make ∘) creating a ComposedConstraint.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ComposedConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ComposedConstraint","text":"ComposedConstraint{T<:AbstractConstraint, U<:AbstractConstraint}\nouter_constraint ∘ inner_constraint\n\nComposing any two AbstractConstraints with ∘ will return this type.\n\nApplies the inner constraint first, then the outer constraint. Checking a ComposedConstraint will check both constraints are satisfied.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"warning: Warning\nAll ComposedConstraints do is apply the two constraints in series and does not do anything intelligent like finding the intersection of the constraints. For this reason, the following three constraints are all different.l1normalize! ∘ nonnegative!\nnonnegative! ∘ l1normalize!\nsimplex!","category":"page"},{"location":"tutorial/constraints/#Custom-Constraints","page":"Constrained Factorization","title":"Custom Constraints","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"You can define your own ProjectedNormalization, ScaledNormalization, or Entrywise constraint using the following constructors.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"ScaledNormalization(norm; whats_normalized=identityslice, scale=1)\nProjectedNormalization(norm, projection; whats_normalized=identityslice)\nEntrywise(apply, check)","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"In fact, these are how the ready-to-use constraints are made. Here are some examples.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"l2scale_1slices! = ScaledNormalization(l2norm; whats_normalized=(x -> eachslice(x; dims=1)))\nl1normalize_rows! = ProjectedNormalization(l1norm, l1project!; whats_normalized=eachrow)\nnonnegative! = Entrywise(ReLU, isnonnegative)\nIntervalConstraint(a, b) = Entrywise(x -> clamp(x, a, b), x -> a <= x <= b)","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"You can also make a custom constraint with GenericConstraint.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.GenericConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.GenericConstraint","text":"GenericConstraint <: AbstractConstraint\n\nGeneral constraint. Simply applies the function apply and checks it was successful with check.\n\nCalling a GenericConstraint on an AbstractArray will use the function in the field apply. Use check(C::GenericConstraint, A) to use the function in the field check.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#Manual-Constraint-Updates","page":"Constrained Factorization","title":"Manual Constraint Updates","text":"","category":"section"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"You can manually define the ConstraintUpdate that gets applied as part of the block decomposition method. These will be automatically inserted into the order of updates immediately following the last update of the matching block with smart_interlace!.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"As an example, if we are performing CP decomposition on an order 3 tensor, the unconstrained block optimization would look something like this.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"BlockUpdate(\n    GradientDescent(1, gradient, step)\n    GradientDescent(2, gradient, step)\n    GradientDescent(3, gradient, step)\n)","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Here the 1, 2, and 3 denote which factor gets updated. If we want to apply a simplex constraint to the second factor, and nonnegative to the 3rd, you can do the following.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"X, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5, constraints=[ConstraintUpdate(3, nonnegative!), Projection(2, simplex!)])","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"This would result in the following block update.","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"BlockUpdate(\n    GradientDescent(1, gradient, step)\n    GradientDescent(2, gradient, step)\n    Projection(2, simplex!)\n    GradientDescent(3, gradient, step)\n    Projection(3, nonnegative!)\n)","category":"page"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"Note the order [ConstraintUpdate(3, nonnegative!), Projection(2, simplex!)] does not matter. Also, using the (abstract) constructor ConstraintUpdate will reduce to the concrete types when possible.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ConstraintUpdate-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ConstraintUpdate","text":"ConstraintUpdate(n, constraint)\n\nConverts an AbstractConstraint to a ConstraintUpdate on the factor n\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/","page":"Constrained Factorization","title":"Constrained Factorization","text":"When using ScaledNormalizations, you may want to manually define what gets rescaled using the downstream ConstraintUpdate concrete type: Rescale.","category":"page"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.Rescale-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.Rescale","text":"Rescale{T<:Union{Nothing,Missing,Function}} <: ConstraintUpdate\nRescale(n, scale::ScaledNormalization, whats_rescaled::T)\n\nApplies the scaled normalization scale to factor n, and tries to multiply the scaling of factor n to other factors.\n\nIf whats_rescaled=nothing, then it will not rescale any other factor.\n\nIf whats_rescaled=missing, then it will try to evenly distribute the weight to all other factors using the (N-1) root of each weight where N is the number of factors. If the weights are not broadcastable, (e.g. you want to scale each row but each factor has a different number of rows), will use the geometric mean of the weights as the single weight to distribute evenly among the other factors.\n\nIf typeof(whats_rescaled) <: Function, will broadcast the weight to the output of calling this function on the entire decomposition. For example,     whats_rescale = x -> eachcol(factor(x, 2)) will rescale each column of the second factor of the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"quickguide/#Quick-Guide","page":"Quick Guide","title":"Quick Guide","text":"","category":"section"},{"location":"quickguide/#Factorizing-your-data","page":"Quick Guide","title":"Factorizing your data","text":"","category":"section"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"The main feature of this package is to factorize an array. This is accomplished with the factorize function.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"Say you've collected your data into an order-3 tensor Y. You can use randn from Random.jl to simulate this.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"using Random\n\nY = randn(100,100,100)","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"Then you can call factorize with a number of keywords. The main keywords you many want to specify are the model and rank. This lets factorize know they type and size of the decomposition. See Decomposition Models for a complete list of avalible models, and how to define your own custom decomposition.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"using BlockTensorFactorization\n\nX, stats, kwargs = factorize(Y; model=Tucker1, rank=5)","category":"page"},{"location":"quickguide/#Extracting-factors","page":"Quick Guide","title":"Extracting factors","text":"","category":"section"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"The main output, X, is of type model (Tucker1). We can call factors to see what the decomposed factors are. Or you can call factor(X,n) to just extract the nth factor. This this case, there are only two factors; the order-3 core (0th factor) G and the matrix (1st factor) A. The following would all be valid ways to extract these factors.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"typeof(X) == Tucker1{Float64, 3}\n\n# Works on any model\nG, A = factors(X)\nG = factor(X, 0)\nA = factor(X, 1)\n\n# Exclusive to Tucker1, Tucker, and CPDecomposition\nG = core(X)\nA = matrix_factors(X)[begin]\n\nsize(G) == (5, 100, 100)\nsize(A) == (100, 5)\nsize(X) == (100, 100, 100)","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"Since the models are subtypes of AbstractArray, all the usual array operations can be performed directly on the decomposition.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"# Difference between a Tucker1 type and a regular Array type\nX - Y\n\n# Entry X_{123} in the tensor\nX[1, 2, 3]","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"If you every want to \"flatten\" the model into a regular Array,  you can call array, or multiply the factors yourself.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"Z = array(X)\ntypeof(Z) == Array{Float64, 3}\n\nG ×₁ A == Z # 1-mode product\nnmode_product(G, A, 1) == Z # 1-mode product\nmtt(A, G) == Z # matrix times tensor","category":"page"},{"location":"quickguide/#Iteration-Statistics","page":"Quick Guide","title":"Iteration Statistics","text":"","category":"section"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"The output variable stats is a DataFrame that records the requested stats every iteration. You can pass a list of supported stats, or custom stats. See Iteration Stats for more details.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"By default, the iteration number, objective value (L2 norm between the input and the model in this case), and the Euclidian norm of the gradient (of the loss function at the current iteration) are recorded. The following would reproduce the default stats in our running example.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"X, stats, kwargs = factorize(Y;\n    stats=[Iteration, ObjectiveValue, GradientNorm]\n)","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"The full results can be displayed in the REPL, or a vector of the individual stat at some or every iteration can be extracted by using the Symbol of that stat (prepend a colon : to the name).","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"display(stats) # Display the full DataFrame\n\nstats[end, :ObjectiveValue] # Final objective value\nstats[:, :ObjectiveValue] # Objective value at every iteration","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"You may also want to see every stat at a particular iteration which can be accessed in the following way. Note that the initilization is stored in the first row, so the nth row stores the stats right before the nth iteration, not after.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"stats[begin, :] # Every stat at the initialization\nstats[4, :] # Every stat right *before* the 4th iteration\nstats[end, :] # Every stat at the final iteration","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"See the DataFrames.jl package for more data handeling.","category":"page"},{"location":"quickguide/#Output-keyword-arguments","page":"Quick Guide","title":"Output keyword arguments","text":"","category":"section"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"Since there are many options and a complicated handeling of defaults arguments, the factorize function also outputs all the keyword arguments as a NamedTuple. This allows you to check what keywords you set, along with the default values that were substituted for the keywords you did not provide.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"You can access the values by getting the relevent field, or index (as a Symbol). In our running example, this would look like the following.","category":"page"},{"location":"quickguide/","page":"Quick Guide","title":"Quick Guide","text":"kwargs.rank == 5\nkwargs[:rank] == 5\ngetfield(kwargs, :rank) == 5\n\nkwarks.model == Tucker1\nkwargs[:model] == Tucker1\ngetfield(kwargs, :model) == Tucker1","category":"page"},{"location":"reference/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Index","title":"Index","text":"The following is a complete list of exported terms in this package.","category":"page"},{"location":"reference/","page":"Index","title":"Index","text":"","category":"page"},{"location":"reference/constants/#Constants","page":"Constants","title":"Constants","text":"","category":"section"},{"location":"reference/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₁-Tuple{AbstractArray, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.:×₁","text":"1-mode product between a tensor and a matrix\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₂","page":"Functions","title":"BlockTensorFactorization.Core.:×₂","text":"2-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₃","page":"Functions","title":"BlockTensorFactorization.Core.:×₃","text":"3-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₄","page":"Functions","title":"BlockTensorFactorization.Core.:×₄","text":"4-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₅","page":"Functions","title":"BlockTensorFactorization.Core.:×₅","text":"5-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₆","page":"Functions","title":"BlockTensorFactorization.Core.:×₆","text":"6-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₇","page":"Functions","title":"BlockTensorFactorization.Core.:×₇","text":"7-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₈","page":"Functions","title":"BlockTensorFactorization.Core.:×₈","text":"8-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₉","page":"Functions","title":"BlockTensorFactorization.Core.:×₉","text":"9-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₁","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₁","text":"Slice-wise dot along mode 1. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₂","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₂","text":"Slice-wise dot along mode 2. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₃","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₃","text":"Slice-wise dot along mode 3. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₄","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₄","text":"Slice-wise dot along mode 4. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₅","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₅","text":"Slice-wise dot along mode 5. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₆","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₆","text":"Slice-wise dot along mode 6. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₇","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₇","text":"Slice-wise dot along mode 7. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₈","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₈","text":"Slice-wise dot along mode 8. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₉","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₉","text":"Slice-wise dot along mode 9. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.DEFAULT_INIT","page":"Functions","title":"BlockTensorFactorization.Core.DEFAULT_INIT","text":"Default initialization function to use when creating a random decomposition.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.Diagonal_col_norm-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.Diagonal_col_norm","text":"Diagonal_col_norm(X)\n\nCalculates a diagonal matrix with entries that are the Euclidean norm of each column of X.\n\nShorthand for Diagonal(norm.(eachcol(X))).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.IntervalConstraint-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.IntervalConstraint","text":"Entrywise(x -> clamp(x, a, b), x -> a ≤ x ≤ b)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.ReLU-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.ReLU","text":"max(0,x)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core._factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core._factorize","text":"Inner level function once keyword arguments are set\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core._gettuckerindex-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core._gettuckerindex","text":"Just computes index I in the tucker product\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.abs_randn-Tuple","page":"Functions","title":"BlockTensorFactorization.Core.abs_randn","text":"abs_randn(x...)\n\nFolded normal or more specificly the half-normal initialization.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.all_recursive-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.all_recursive","text":"all_recursive(x)\nall_recursive(f, x)\n\nLike all but checks recursively on nested types like arrays of vectors, tuples of sets of arrays, etc.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.array-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.array","text":"array(D::AbstractDecomposition)\n\nTurns a decomposition into the full array, usually by multiplying the factors to reconstruct the full array.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.binary!","page":"Functions","title":"BlockTensorFactorization.Core.binary!","text":"Entrywise(binaryproject, x -> x in (0, 1))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.check-Tuple{AbstractConstraint, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.check","text":"check(C::AbstractConstraint, A::AbstractArray)::Bool\n\nReturns true if A satisfies the constraint C.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.check-Tuple{Entrywise, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.check","text":"check(C::Entrywise, A::AbstractArray)::Bool\n\nChecks if A is entrywise constrained\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.circumscribed_radius-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.circumscribed_radius","text":"Finds the radius of the circumscribed circle between points (a,f), (b,g), (c,h)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.coarsen-Tuple{AbstractArray, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.coarsen","text":"coarsen(Y::AbstractArray, scale::Integer; dims=1:ndims(Y))\n\nCoarsens or downsamples Y by scale. Only keeps every scale entries along the dimensions specified.\n\nExample\n\nY = randn(12, 12, 12)\n\ncoarsen(Y, 2) == Y[begin:2:end, begin:2:end, begin:2:end]\n\ncoarsen(Y, 4; dims=(1, 3)) == Y[begin:4:end, :, begin:4:end]\n\ncoarsen(Y, 3; dims=2) == Y[:, begin:3:end, :]\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.contractions-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.contractions","text":"contractions(D::AbstractDecomposition)\n\nA tuple of functions defining a recipe for reconstructing a full array from the factors of the decomposition.\n\nExample\n\n(op1, op2) = contractions(D) (A, B, C) = factors(D)\n\narray(D) == (A op1 B) op2 C\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.core-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.core","text":"core(T::AbstractTucker)\n\nThe core of a Tucker-like decomposition. Same number of dimensions as the full array.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.cpproduct-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.cpproduct","text":"cpproduct((A, B, C, ...))\ncpproduct(A, B, C, ...)\n\nMultiplies the inputs by treating them as matrices in a CP decomposition.\n\nExample\n\ncpproduct(A, B, C) == @einsum T[i, j, k] := A[i, r] * B[j, r] * C[k, r]\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.cubic_spline_coefficients-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.cubic_spline_coefficients","text":"cubic_spline_coefficients(y::AbstractVector{<:Real}; h=1)\n\nCalculates the list of coefficients a, b, c, d for an interpolating spline.\n\nThe spline is defined as f(x) = g_i(x) on xi leq x leq xi+1 where\n\ng_i(x) = ai(x-xi)^3 + bi(x-xi)^2 + ci(x-xi) + di\n\nUses the following boundary conditions\n\ng_1(x1-h) = 1 (i.e. the y-intercept is (01) for uniform spaced x=1:n)\ng_n(xn+h) = xn (i.e. repeated right end-point)\ng_n(xn+h) = 0 (i.e. flat/no-curvature one spacing after end-point)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.curvature-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.curvature","text":"curvature(y::AbstractVector{<:Real})\n\nApproximates the signed curvature of a function given evenly spaced samples.\n\nUses d_dx and d2_dx2 to approximate the first two derivatives.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d2_dx2-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d2_dx2","text":"d2_dx2(y::AbstractVector{<:Real})\n\nApproximate second derivative with finite elements. Assumes y[i] = y(xi) are samples with unit spaced inputs x{i+1} - x_i = 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d_dx-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d_dx","text":"d_dx(y::AbstractVector{<:Real})\n\nApproximate first derivative with finite elements. Assumes y[i] = y(xi) are samples with unit spaced inputs x{i+1} - x_i = 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d_dx_and_d2_dx2_spline-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d_dx_and_d2_dx2_spline","text":"Extracts the first and second derivatives of the splines at the knots\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.default_kwargs-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.default_kwargs","text":"default_kwargs(Y; kwargs...)\n\nHandles all keywords and options, and sets defaults if not provided.\n\nKeywords & Defaults\n\nInitialization\n\ndecomposition: nothing. Can provide a custom initialized AbstractDecomposition. Note this exact decomposition is mutated.\nmodel: Tucker1, but overridden by the type of AbstractDecomposition if given decomposition\nrank: nothing, but overridden by the rank of AbstractDecomposition if given decomposition. Automatically calls rank_detect_factorize if both rank & decomposition are not provided.\ninit: abs_randn for nonnegative inputs Y, randn otherwise\nconstrain_init: true. Ensures the initialization satisfies all given constraints. Defaults to false if given decomposition\nfreeze: the default frozen factors of the model\ncontinuous_dims: missing. Dimensions of Y that come from discretizations of continuous data. If provided, multiscale_factorize is called and can speed up factorization. If continuous_dims==nothing, factorization will only happen at one scale. In the future, if continuous_dims==missing,factorize may guess if there are continuous dimensions.\n\nUpdates\n\nobjective: L2(). Objective to minimize\nnorm: l2norm. Norm to use for statistics, can be unrelated to the objective\nrandom_order: false. Perform the updates in a random order each iteration, Overrides to true when recursive_random_order=true\ngroup_updates_by_factor: false. Groups updates on the same factor together. Overrides to true when random_order=true. Useful when randomizing order of updates but you want to keep matching momentum-gradientstep-constraint together\nrecursive_random_order: false. Performs inner blocked updates (grouped updates) in a random order (recursively) each iteration. Note the outer most list of updates can be performed in order if random_order=false\ndo_subblock_updates: false. Performs gradient descent on subblocks within a factor separately. May result in smaller Lipschitz constants and hence larger step sizes being used.\n\nMomentum\n\nmomentum: true\nδ: 0.9999. Amount of momentum, between [0,1)\nprevious_iterates: 1. Number of pervious iterates to save and use between iterations\n\nConstraints\n\nconstraints: nothing. Can be a list of ConstraintUpdate, or just one\nfinal_constraints: nothing. Constraints to apply after the final iteration. Will apply constraints if constrain_output is true and none are given\nconstrain_output: false. Apply the final_constraints, will override to true if final_constraints are given\n\nStats\n\nstats: [Iteration, ObjectiveValue, GradientNorm] or in the case of nonnegative Y, GradientNNCone in place of GradientNorm\nconverged: GradientNorm or in the case of nonnegative Y, GradientNNCone. What stat(s) to use for convergence. Will converge is any one of the provided stats is below their respective tolerance\ntolerance: 1. A list the same length as converged\nmaxiter: 1000. Additional stopping criterion if the number of iterations exceeds this number\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachfactorindex-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.eachfactorindex","text":"eachfactorindex(D::AbstractDecomposition)\n\nAn iterable to index the factors of D; not necessarily 1:ndims(D). Does not include non-data factors like the core of a CPDecomposition.\n\nFor example, eachfactorindex(D::Tucker) == 0:ndims(D) since core(D) is the zeroth factor. eachfactorindex(D::CPDecomposition) == 1:ndims(D). core(D) exists, but it is frozen as the identity tensor. And eachfactorindex(D::Tucker1) == 0:1 representing the core and matrix factor of D.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachfibre-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.eachfibre","text":"eachfibre(A::AbstractArray; n::Integer, kwargs...)\n\nCreates views of A that are that n-fibres of A.\n\nShorthand for eachslice(A; dims=(1,...,n-1,n+1,...,ndims(A))).\n\nSee eachslice.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::AbstractTucker)\n\nCreates a generator for each rank 1 term of a Tucker decomposition.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::CPDecomposition)\n\nThe (CP) rank-1 tensors Tr[i1, ..., iN] = A1[i1, r] * ... * AN[iN, r]  for each r = 1, ..., rankof(T).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{Tucker1}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::Tucker1)\n\nThe (Tucker-1) rank-1 tensors Tr[i1, ..., iN] = A[i1,r] * B[r, i2, ..., iN] for each r = 1, ..., rankof(T).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.expand_decomposition_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.expand_decomposition_constraints","text":"Use the same initialization as factorize() to get the expanded set of constraints\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factor-Tuple{AbstractDecomposition, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.factor","text":"factor(D::AbstractDecomposition, n::Integer)\n\nThe nth factor of D.\n\nUse factors to get all the factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.factorize","text":"factorize(Y; rank=nothing, model=Tucker1, kwargs...)\n\nFactorizes Y according to the decomposition model.\n\nSee default_kwargs for the default keywords.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factors-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.factors","text":"factors(D::AbstractDecomposition)\n\nA tuple of arrays representing the decomposition of D.\n\nUse factor to get just the nth factor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.false_tuple-Tuple{Integer}","page":"Functions","title":"BlockTensorFactorization.Core.false_tuple","text":"Makes a Tuple of length n filled with false.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.finalconstrain!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.finalconstrain!","text":"finalconstrain!(decomposition; constraints, final_constraints, kwargs...)\n\nApplies final_constraints (or if its nothing, applies constraints) to the decomposition.\n\nAny RescaleUpdate are applied (the factor is scaled), but the rescaling of other factors is skipped.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.frozen-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.frozen","text":"frozen(D::AbstractDecomposition)\n\nA tuple of Bools the same length as factors(D) showing which factors are \"frozen\" in the sense that a block decent algorithm should skip these factors when decomposing a tensor.\n\nSee isfrozen.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.geomean-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.geomean","text":"geomean(v)\ngeomean(v...)\n\nGeometric mean of a collection: prod(v)^(1/length(v)).\n\nIf prod(v) is detected to be 0 or Inf, the safer (but slower) implementation exp(mean(log.(v))) is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.getnotindex-Tuple{Any, Int64}","page":"Functions","title":"BlockTensorFactorization.Core.getnotindex","text":"Like getindex but returns the compliment to the index or indices requested.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.group_by_factor-Tuple{BlockedUpdate}","page":"Functions","title":"BlockTensorFactorization.Core.group_by_factor","text":"group_by_factor(blockedupdate::BlockedUpdate)\n\nGroups updates according to the factor they operate on.\n\nIf blockedupdate contains other BlockedUpdates, the inner updates are grouped when they all operate on the same factor.\n\nUpdates which do not have an assigned factor are grouped together.\n\nThe order which these groups appear in the output follows the same order as the first appearence of each unique factor that is operated on.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.identity_tensor-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.identity_tensor","text":"identity_tensor(I, ndims)\nidentity_tensor(T, I, ndims)\n\nCreates a SuperDiagonal array of ones with size I × ... × I of order ndims.\n\nCan provide a type T for the identity tensor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.identityslice-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Functions","title":"BlockTensorFactorization.Core.identityslice","text":"identityslice(x::AbstractArray{T, N})\n\nUseful for returning an iterable with a single iterate x\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize","text":"Main initialization function for factorize.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_continuous_dims-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_continuous_dims","text":"initialize_continuous_dims(Y; kwargs...)\n\nLists dimensions of Y that represent a discretization of a continuous function.\n\nDefaults to all of them: continuous_dims = 1:ndims(Y).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_decomposition-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_decomposition","text":"The decomposition model Y will be factored into\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_parameters-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_parameters","text":"update parameters needed for the update\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_previous-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_previous","text":"Keep track of one or more previous iterates\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_scales-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_scales","text":"initialize_scales(Y, kwargs)\n\nInitializes the plan for factorizing at progressively finer scales.\n\nThe list of scales should be ordered from largest (coarse) to smallest (fine) and end with 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_stats-NTuple{4, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_stats","text":"The stats that will be saved every iteration\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.interlace-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.interlace","text":"interlace(u, v)\n\nTakes two iterables, u and v, and alternates elements from u and v into a vector. If u and v are not the same length, extra elements are put on the end of the vector.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.interpolate-Tuple{AbstractArray, Any}","page":"Functions","title":"BlockTensorFactorization.Core.interpolate","text":"interpolate(Y, scale; dims=1:ndims(Y), degree=0, kwargs...)\n\nInterpolates Y to a larger array with repeated values.\n\nKeywords\n\nscale. How much to scale up the size of Y. A dimension with size k will be scaled to scale*k - (scale - 1) = scale*(k-1) + 1\n\ndims:1:ndims(Y). Which dimensions to interpolate.\n\ndegree:0. What degree of interpolation to use. 0 is constant interpolation, 1 is linear.\n\nLike the opposite of coarsen.\n\nExample\n\njulia> Y = collect(reshape(1:6, 2, 3)) 2×3 Matrix{Int64}:  1  3  5  2  4  6\n\njulia> interpolate(Y, 2) 3×5 Matrix{Int64}:  1  1  3  3  5  1  1  3  3  5  2  2  4  4  6\n\njulia> interpolate(Y, 3; dims=2) 2×7 Matrix{Int64}:  1  1  1  3  3  3  5  2  2  2  4  4  4  6\n\njulia> interpolate(Y, 1) == Y true\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isfrozen-Tuple{AbstractDecomposition, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.isfrozen","text":"isfrozen(D::AbstractDecomposition, n::Integer)\n\nTrue if the nth factor of D is frozen. See frozen.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isnonnegative-Tuple{AbstractArray{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.isnonnegative","text":"isnonnegative(X::AbstractArray{<:Real})\nisnonnegative(x::Real)\n\nChecks if all entries of X are bigger or equal to zero. Will be a standard function in Base but using this for now: https://github.com/JuliaLang/julia/pull/53677\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isnonnegative_sumtoone-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.isnonnegative_sumtoone","text":"all(isnonnegative, x) && sum(x) ≈ 1\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.khatrirao-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.khatrirao","text":"khatrirao(A::AbstractMatrix, B::AbstractMatrix)\nA ⊙ B\n\nKhatri-Rao product of two matrices. A ⊙ B can be typed with \\odot.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize!","text":"ProjectedNormalization(l1norm, l1project!)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_12slices!","text":"ProjectedNormalization(l1norm, l1project!; whats_normalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_1slices!","text":"ProjectedNormalization(l1norm, l1project!; whats_normalized=(x -> eachslice(x; dims=1)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_cols!","text":"ProjectedNormalization(l1norm, l1project!; whats_normalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_rows!","text":"ProjectedNormalization(l1norm, l1project!; whats_normalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale!","text":"ScaledNormalization(l1norm)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_12slices!","text":"ScaledNormalization(l1norm; whats_normalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_1slices!","text":"ScaledNormalization(l1norm; whats_normalized=(x -> eachslice(x; dims=1)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_average12slices!","text":"ScaledNormalization(l1norm; whats_normalized=(x -> eachslice(x; dims=1)), scale=(A -> size(A, 2)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_cols!","text":"ScaledNormalization(l1norm; whats_normalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_rows!","text":"ScaledNormalization(l1norm; whats_normalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale!","text":"ScaledNormalization(l2norm)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_12slices!","text":"ScaledNormalization(l2norm; whats_normalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_1slices!","text":"ScaledNormalization(l2norm; whats_normalized=(x -> eachslice(x; dims=1)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_average12slices!","text":"ScaledNormalization(l2norm; whats_normalized=(x -> eachslice(x; dims=1)), scale=(A -> size(A, 2)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_cols!","text":"ScaledNormalization(l2norm; whats_normalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_rows!","text":"ScaledNormalization(l2norm; whats_normalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynorm-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.linftynorm","text":"linftynorm(x::AbstractArray)\n\nCalculates ‖x‖_∞.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize!","text":"ProjectedNormalization(linftynorm, linftyproject!)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_12slices!","text":"ProjectedNormalization(linftynorm, linftyproject!;     whats_normalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_1slices!","text":"ProjectedNormalization(linftynorm, linftyproject!;     whats_normalized=(x -> eachslice(x; dims=1))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_cols!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_cols!","text":"ProjectedNormalization(linftynorm, linftyproject!; whats_normalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_rows!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_rows!","text":"ProjectedNormalization(linftynorm, linftyproject!; whats_normalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyproject!-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.linftyproject!","text":"linftyproject!(x::AbstractArray)\n\nEuclidean projection onto the unit L∞-ball.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale!","text":"ScaledNormalization(linftynorm)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_12slices!","text":"ScaledNormalization(linftynorm; whats_normalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_1slices!","text":"ScaledNormalization(linftynorm; whats_normalized=(x -> eachslice(x; dims=1)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_average12slices!","text":"ScaledNormalization(linftynorm; whats_normalized=(x -> eachslice(x; dims=1)), scale=(A -> size(A, 2)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_cols!","text":"ScaledNormalization(linftynorm; whats_normalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_rows!","text":"ScaledNormalization(linftynorm; whats_normalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.make_spline-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.make_spline","text":"make_spline(y::AbstractVector{<:Real}; h=1)\n\nReturns a function f(x) that is an interpolating/extrapolating spline for y, with uniform stepsize h between the x-values of the knots.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.make_update!-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.make_update!","text":"What one iteration of the algorithm looks like. One iteration is likely a full cycle through each block or factor of the model.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.mat-Tuple{AbstractArray, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.mat","text":"mat(A::AbstractArray, n::Integer)\n\nMatricize along the nth mode.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_cols!-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.match_cols!","text":"match_cols!(X::AbstractMatrix, Y::AbstractMatrix; kwargs...)\n\nmatch_slices! along the second dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_factors!-Union{Tuple{T}, Tuple{T, T}} where T<:AbstractDecomposition","page":"Functions","title":"BlockTensorFactorization.Core.match_factors!","text":"match_factors!(X::T, Y::T; dist=L2) where {T <: AbstractDecomposition}\n\nReorders the rank-1 terms of X to best match the slices of Y.\n\nThis is currently implemented for Tucker1 and CPDecomposition.\n\nSee match_slices!.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_rows!-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.match_rows!","text":"match_rows!(X::AbstractMatrix, Y::AbstractMatrix; kwargs...)\n\nmatch_slices! along the first dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_slices!-Tuple{AbstractArray, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.match_slices!","text":"match_slices!(X, Y; dims, dist=L2)\n\nReorders the order-dims slices of X to best match the slices of Y.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.matrix_factor-Tuple{AbstractTucker, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.matrix_factor","text":"matrix_factor(T::AbstractTucker, n::Integer)\n\nThe nth matrix factor. See matrix_factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.matrix_factors-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.matrix_factors","text":"matrix_factors(T::AbstractTucker)\n\nTuple of the non-core factors of T. See matrix_factor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.mtt-Tuple{AbstractMatrix, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.mtt","text":"Matrix times Tensor\n\nLooks like C[i1, i2, ..., iN] = ∑_r A[i1, r] * B[r, i2, ..., iN] entry-wise.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.multifoldl-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.multifoldl","text":"multifoldl(ops, args)\n\nLike foldl, but with a different folding operation between each argument.\n\nExample\n\njulia> multifoldl((+,*,-), (2,3,4,5)) 15\n\njulia> ((2 + 3) * 4) - 5 15\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.multiscale_factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.multiscale_factorize","text":"multiscale_factorize(Y; continuous_dims=1:ndims(Y), rank=1, model=Tucker1, kwargs...)\n\nLike factorize but uses progressively finer sub-grids of Y to speed up convergence. This is only effective when the dimensions given by dims come from discretizations of continuous data.\n\nFor example, if Y has 3 dimensions where Y[i, j, k] are samples from a continuous 2D function fi(xj, yk) on a grid, use `multiscalefactorize(Y; continuous_dims=(2,3))` since second and third dimensions are continuous.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nfactors-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.nfactors","text":"nfactors(D::AbstractDecomposition)\n\nReturns the number of factors/blocks in a decomposition.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmode_product-Tuple{AbstractArray, AbstractMatrix, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.nmode_product","text":"nmode_product(A::AbstractArray, B::AbstractMatrix, n::Integer)\n\nContracts the nth mode of Awith the first mode ofB. Equivalent toA×ₙB` where\n\n(A ×ₙ B)[i₁, …, iN] = ∑ⱼ A[i₁, …, iₙ₋₁, j, iₙ₊₁, …, iN] B[iₙ, j].\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmode_product-Tuple{AbstractArray, AbstractVector, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.nmode_product","text":"nmode_product(A::AbstractArray, b::AbstractVector, n::Integer)\n\nContracts the nth mode of A with b. Equivalent to A ×ₙ b where\n\n(A ×ₙ b)[i₁, …, iₙ₋₁, iₙ₊₁, …, iN] = ∑iₙ A[i₁, …, i_N] b[iₙ].\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmp","page":"Functions","title":"BlockTensorFactorization.Core.nmp","text":"Shorthand for nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.nonnegative!","page":"Functions","title":"BlockTensorFactorization.Core.nonnegative!","text":"Entrywise(ReLU, isnonnegative)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.norm2-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.norm2","text":"norm2(x)\n\nL2 norm squared, the sum of squares of the entries of x.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.parse_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.parse_constraints","text":"parse_constraints(constraints, decomposition; kwargs...)\n\nParses the constraints to make sure we have a valid list of ConstraintUpdate.\n\nIf only one AbstractConstraint is given, assume we want this constraint to apply to every factor in the decomposition, and make a ConstraintUpdate for each factor.\n\nIf we are given a list of AbstractConstraint, assume we want them to apply to each factor of the decomposition in order.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.possible_ranks-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.possible_ranks","text":"possible_ranks(Y, model)\n\nReturns the rank of possible ranks Y could have under the model.\n\nFor matrices I×J this is 1:min(I, J). This is can be extended to tensors for different type of decompositions.\n\nTucker-1 rank is ≤ min(I, prod(J1,...,JN)) for tensors I×J1×...×JN.\n\nThe CP-rank is ≤ minimum_{n} (prod(I1,...,IN) / In) for tensors I1×...×IN in general. Although some shapes have have tighter upper bounds. For example, 2×I×I tensors over ℝ have a maximum rank of floor(3I/2).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.postprocess!-NTuple{8, Any}","page":"Functions","title":"BlockTensorFactorization.Core.postprocess!","text":"Any post algorithm processing that needs to be done in factorize.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.proj_one_hot!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.proj_one_hot!","text":"proj_one_hot!(x)\n\nMutating version of proj_one_hot.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.proj_one_hot-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.proj_one_hot","text":"proj_one_hot(x)\n\nProjects an array x to the closest one hot array: an array with all 0's except for a single 1. Does not mutate; see proj_one_hot! for a mutating version.\n\nThis is not a unique projection if there are multiple largest entries. In this case, will pick one of the largest entries to be 1 and set the rest to zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.projsplx!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.projsplx!","text":"projsplx!(y; sum=one(eltype(y)))\n\nProjects (in Euclidian distance) the array y into the simplex.\n\nSee projsplx for a non-mutating version.\n\n[1] Yunmei Chen and Xiaojing Ye, \"Projection Onto A Simplex\", 2011\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.projsplx-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.projsplx","text":"projsplx(y; sum=one(eltype(y)))\n\nNon-mutating version of projsplx!.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rank_detect_factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.rank_detect_factorize","text":"rank_detect_factorize(Y; online_rank_estimation=false, rank=nothing, model=Tucker1, kwargs...)\n\nWraps factorize() with rank detection.\n\nSelects the rank that maximizes the standard curvature of the Relative Error (as a function of rank).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rankof-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.rankof","text":"rankof(D::AbstractDecomposition)\n\nInternal dimension sizes for a decomposition. Returns the sizes of all factors if not defined for a concrete subtype of AbstractDecomposition.\n\nExamples\n\nCPDecomposition: size of second dimension for the factors Tucker: size of the core factor Tucker1: size of the first dimension of the core factor\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rankof-Tuple{CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.rankof","text":"The single rank for a CP Decomposition\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.reshape_ndims-Tuple{AbstractArray, Any}","page":"Functions","title":"BlockTensorFactorization.Core.reshape_ndims","text":"reshape_ndims(x, n)\n\nReshapes x to a higher order array with n dimensions.\n\nWhen n > ndims(x), extra dimensions are prepended. Otherwise, trailing dimensions are collapsed.\n\nExample\n\njulia> x = [1, 2, 3] 3-element Vector{Int64}:  1  2  3\n\njulia> reshape_ndims(x, 1) 3-element Vector{Int64}:  1  2  3\n\njulia> reshape_ndims(x, 2) 1×3 Matrix{Int64}:  1  2  3\n\njulia> reshape_ndims(x, 3) 1×1×3 Array{Int64, 3}: [:, :, 1] =  1\n\n[:, :, 2] =  2\n\n[:, :, 3] =  3\n\njulia> A = reshape(collect(1:12), 2,2,3) 2×2×3 Array{Int64, 3}: [:, :, 1] =  1  3  2  4\n\n[:, :, 2] =  5  7  6  8\n\n[:, :, 3] =   9  11  10  12\n\njulia> reshape_ndims(A, 2) 2×6 Matrix{Int64}:  1  3  5  7   9  11  2  4  6  8  10  12\n\nCredit: https://discourse.julialang.org/t/outer-product-broadcast/103731/7\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_constraint-Tuple{AbstractConstraint, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.scale_constraint","text":"scale_constraint(constraint::AbstractConstraint, scale, n_continuous_dims)\n\nReturns a scaled version of the constraint based off the number of relevant continuous dimensions the constraint acts on.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.scale_constraints","text":"scale_constraints(Y, scale; kwargs...)\n\nScales any constraints that need to be modified to use at a coarser scale.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_decomposition_constraint-Tuple{Any, Any, Any, CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.scale_decomposition_constraint","text":"Idea is that external dimensions (I₁, I₂, ...) that are continuous dimensions need to be scaled, but internal dimensions (R₁, R₂, ...) or non-continuous dimensions don't.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex!","page":"Functions","title":"BlockTensorFactorization.Core.simplex!","text":"ProjectedNormalization(isnonnegative_sumtoone, projsplx!)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_12slices!","text":"ProjectedNormalization(isnonnegativesumtoone, projsplx!; whatsnormalized=(x -> eachslice(x; dims=(1,2))))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_1slices!","text":"ProjectedNormalization(isnonnegativesumtoone, projsplx!; whatsnormalized=(x -> eachslice(x; dims=1)))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_cols!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_cols!","text":"ProjectedNormalization(isnonnegativesumtoone, projsplx!; whatsnormalized=eachcol)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_rows!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_rows!","text":"ProjectedNormalization(isnonnegativesumtoone, projsplx!; whatsnormalized=eachrow)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.slicewise_dot-Tuple{AbstractArray, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.slicewise_dot","text":"slicewise_dot(A::AbstractArray, B::AbstractArray; dims=1, dimsA=dims, dimsB=dims)\n\nContracts all but the dimensions dimsA and dimsB of A and B. Equivalent to A ⋅ₙ B where\n\n(A ⋅ₙ B)[idimsA, idimsB] = A[…, idimsA, ⋯] ⋅ B[…, idimsB, ⋯].\n\nFor example, if A and B are both matrices, slicewise_dot(A, B) == A*B'.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.smart_insert!-Tuple{BlockedUpdate, AbstractUpdate}","page":"Functions","title":"BlockTensorFactorization.Core.smart_insert!","text":"smart_insert!(U::BlockedUpdate, V::AbstractUpdate)\n\nTries to insert V into U after the last matching update in U. A \"matching update\" means it updates the same factor/block n. See smart_interlace!\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.smart_interlace!-Tuple{BlockedUpdate, Any}","page":"Functions","title":"BlockTensorFactorization.Core.smart_interlace!","text":"smart_interlace!(U::BlockedUpdate, V)\n\nsmart_insert!s each update in V, into U. See smart_insert!\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.spline_mat-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.spline_mat","text":"spline_mat(n)\n\nCreates the Tridiagonal matrix to solve for coefficients b.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.standard_curvature-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.standard_curvature","text":"standard_curvature(y::AbstractVector{<:Real})\n\nApproximates the signed curvature of a function, scaled to the unit box 01^2.\n\nSee curvature.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.swapdims","page":"Functions","title":"BlockTensorFactorization.Core.swapdims","text":"swapdims(A::AbstractArray, a::Integer, b::Integer=1)\n\nSwap dimensions a and b.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.tucker_contractions-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.tucker_contractions","text":"tucker_contractions(N)\n\nContractions used in a full Tucker decomposition of order N. This is a tuple of the n-mode products from 1 to N in order.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.tuckerproduct-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.tuckerproduct","text":"tuckerproduct(G, (A, B, ...))\ntuckerproduct(G, A, B, ...)\n\nMultiplies the inputs by treating the first argument as the core and the rest of the arguments as matrices in a Tucker decomposition.\n\nExample\n\ntuckerproduct(G, (A, B, C)) == G ×₁ A ×₂ B ×₃ C tuckerproduct(G, (A, B, C); exclude=2) == G ×₁ A ×₃ C tuckerproduct(G, (A, B, C); exclude=2, excludesmissing=false) == G ×₁ A ×₃ C tuckerproduct(G, (A, C); exclude=2, excludesmissing=true) == G ×₁ A ×₃ C\n\n\n\n\n\n","category":"method"},{"location":"tutorial/decompositionmodels/#Decomposition-Models","page":"Decomposition Models","title":"Decomposition Models","text":"","category":"section"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"The main abstract type for all decomposition models is AbstractDecomposition.","category":"page"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.AbstractDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.AbstractDecomposition","text":"Abstract type for the different decompositions.\n\nMain interface for subtypes are the following functions. Required:     array(D): how to construct the full array from the decomposition representation     factors(D): a tuple of arrays, the decomposed factors Optional:     getindex(D, i::Int) and getindex(D, I::Vararg{Int, N}): how to get the ith or Ith         element of the reconstructed array. Defaults to getindex(array(D), x), but there         is often a more efficient way to get a specific element from large tensors.     size(D): Defaults to size(array(D)).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"This can be subtyped to create your own decomposition model.","category":"page"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"The following common tensor models are available as valid arguments to model and built into this package.","category":"page"},{"location":"tutorial/decompositionmodels/#Tucker-types","page":"Decomposition Models","title":"Tucker types","text":"","category":"section"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.Tucker-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker((G, A, B, ...))\nTucker((G, A, B, ...), frozen)\n\nTucker decomposition. Takes the form of a core G times a matrix for each dimension.\n\nFor example, a rank (r, s, t) Tucker decomposition of an order three tensor D would be, entry-wise, D[i, j, k] = ∑r ∑s ∑_t G[r, s, t] * A[i, r] * B[j, s] * C[k, t].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee tuckerproduct.\n\n\n\n\n\nTucker(full_size::NTuple{N, Integer}, ranks::NTuple{N, Integer};\n    frozen=false_tuple(length(ranks)+1), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker type using init to initialize the factors.\n\nSee Tucker1.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.Tucker1-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1((G, A))\nTucker1((G, A), frozen)\n\nTucker-1 decomposition. Takes the form of a core G times a matrix A. Entry-wise\n\nD[i₁, …, iN] = ∑r G[r, i₂, …, i_N] * A[i₁, r].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee ×₁ and mtt.\n\n\n\n\n\nTucker1(full_size::NTuple{N, Integer}, rank::Integer; frozen=false_tuple(2), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker1 type using init to initialize the factors.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.CPDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.CPDecomposition","text":"CP decomposition. Takes the form of an outerproduct of multiple matrices.\n\nFor example, a rank r CP decomposition of an order three tensor D would be, entry-wise, D[i, j, k] = sum_r A[i, r] * B[j, r] * C[k, r]).\n\nCPDecomposition((A, B, C))\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"Note these are all subtypes of AbstractTucker.","category":"page"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.AbstractTucker-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.AbstractTucker","text":"Abstract type for all Tucker-like decompositions. AbstractTucker decompositions have a core with the same number of dimensions as the full array, and (a) matrix factor(s).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#Other-types","page":"Decomposition Models","title":"Other types","text":"","category":"section"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.GenericDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.GenericDecomposition","text":"Most general decomposition. Takes the form of interweaving contractions between the factors.\n\nFor example, T = A * B + C could be represented as GenericDecomposition((A, B, C), (*, +))\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.SingletonDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.SingletonDecomposition","text":"SingletonDecomposition(A::AbstractArray, frozen=false)\n\nWraps an AbstractArray so it can be treated like an AbstractDecomposition\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#How-Julia-treats-an-AbstractDecomposition","page":"Decomposition Models","title":"How Julia treats an AbstractDecomposition","text":"","category":"section"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"AbstractDecomposition is an abstract subtype of AbstractArray. AbstractDecomposition will keep track of the element type and number of dimensions like other AbstractArray. This is the T and N in the type Array{T,N}. To make AbstractDecomposition behave like other array types, Julia only needs to know how to access/compute indexes of the array through getindex. These indices are computed on the fly when a particular index is requested, or the whole tensor is computed from its factors through array(X). This has the advantage of minimizing the memory used, and allows for the most flexibility since any operation that is supported by AbstractArray will work on AbstractDecomposition types. The drawback is that repeated requests to entries must recompute the entry each time. In these cases, it is best to \"flatten\" the array with array(X) first before making these repeated calls.","category":"page"},{"location":"tutorial/decompositionmodels/","page":"Decomposition Models","title":"Decomposition Models","text":"Some basic operations like +, -, *, \\, and / will either compute the operation is some optimized way, or call array(X) function to first flatten the decomposition into a regular Array type in some optimized way. Operations that don't have an optimized method (because I can only do so much), will instead call Julia's Array{T,N}(X) to convert the model into a regular Array{T,N} type. This is usually slower and less memory efficient since it calls getindex on every index individually, instead of computing the whole array at once.","category":"page"},{"location":"reference/types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractConstraint","page":"Types","title":"BlockTensorFactorization.Core.AbstractConstraint","text":"Abstract parent type for the various constraints\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractDecomposition","page":"Types","title":"BlockTensorFactorization.Core.AbstractDecomposition","text":"Abstract type for the different decompositions.\n\nMain interface for subtypes are the following functions. Required:     array(D): how to construct the full array from the decomposition representation     factors(D): a tuple of arrays, the decomposed factors Optional:     getindex(D, i::Int) and getindex(D, I::Vararg{Int, N}): how to get the ith or Ith         element of the reconstructed array. Defaults to getindex(array(D), x), but there         is often a more efficient way to get a specific element from large tensors.     size(D): Defaults to size(array(D)).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractObjective","page":"Types","title":"BlockTensorFactorization.Core.AbstractObjective","text":"AbstractObjective <: Function\n\nGeneral interface is\n\nstruct L2 <: AbstractObjective end\n\nafter constructing\n\nmyobjective = L2()\n\nyou can call\n\nmyobjective(X, Y)\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractStat","page":"Types","title":"BlockTensorFactorization.Core.AbstractStat","text":"An AbstractStat is a type which, when created, can be applied to the four arguments (X::AbstractDecomposition, Y::AbstractArray, previous::Vector{<:AbstractDecomposition}, parameters::Dict) to (usually) return a number.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractStep","page":"Types","title":"BlockTensorFactorization.Core.AbstractStep","text":"Interface to make a step scheme is\n\nstruct MyStep <: AbstractStep     ... end\n\nfunction (step::MyStep)(x::AbstractDecomposition; kwargs...)     ...     return step::Real end\n\nTo use your scheme, construct an instance with any necessary parameters\n\nmystep = MyStep(...)\n\nand then you can call\n\nstep = mystep(D; kwargs...)\n\nto compute the step size.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractTucker","page":"Types","title":"BlockTensorFactorization.Core.AbstractTucker","text":"Abstract type for all Tucker-like decompositions. AbstractTucker decompositions have a core with the same number of dimensions as the full array, and (a) matrix factor(s).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.BlockGradientDescent","page":"Types","title":"BlockTensorFactorization.Core.BlockGradientDescent","text":"Perform a Block Gradient decent step on the nth factor of an Abstract Decomposition x\n\nThe n is only to keep track of the factor that gets updated, and to check if a frozen factor was requested to be updated.\n\nThis type allows for more complicated step sizes such as individual steps for sub-blocks of the nth factor.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.CPDecomposition","page":"Types","title":"BlockTensorFactorization.Core.CPDecomposition","text":"CP decomposition. Takes the form of an outerproduct of multiple matrices.\n\nFor example, a rank r CP decomposition of an order three tensor D would be, entry-wise, D[i, j, k] = sum_r A[i, r] * B[j, r] * C[k, r]).\n\nCPDecomposition((A, B, C))\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ComposedConstraint","page":"Types","title":"BlockTensorFactorization.Core.ComposedConstraint","text":"ComposedConstraint{T<:AbstractConstraint, U<:AbstractConstraint}\nouter_constraint ∘ inner_constraint\n\nComposing any two AbstractConstraints with ∘ will return this type.\n\nApplies the inner constraint first, then the outer constraint. Checking a ComposedConstraint will check both constraints are satisfied.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ConstraintUpdate-Tuple{Any, AbstractConstraint}","page":"Types","title":"BlockTensorFactorization.Core.ConstraintUpdate","text":"ConstraintUpdate(n, constraint)\n\nConverts an AbstractConstraint to a ConstraintUpdate on the factor n\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.DiagonalTuple","page":"Types","title":"BlockTensorFactorization.Core.DiagonalTuple","text":"Alias for NTuple{N, Diagonal{T}}\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.DisplayDecomposition","page":"Types","title":"BlockTensorFactorization.Core.DisplayDecomposition","text":"DisplayDecomposition(; kwargs...)\n\nDoes not use any of the kwargs. Simply displays the current iteration.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Entrywise","page":"Types","title":"BlockTensorFactorization.Core.Entrywise","text":"Entrywise constraint. Note both apply and check needs to be performed entrywise on an array\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Entrywise-Tuple{AbstractArray}","page":"Types","title":"BlockTensorFactorization.Core.Entrywise","text":"Make entrywise callable, by applying the constraint entrywise to arrays\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.EuclidianLipschitz","page":"Types","title":"BlockTensorFactorization.Core.EuclidianLipschitz","text":"The 2-norm of the lipschitz constants that would be taken for all blocks.\n\nNeed the stepsizes to be lipschitz steps since it is calculated similarly to EuclidianStepSize.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.EuclidianStepSize","page":"Types","title":"BlockTensorFactorization.Core.EuclidianStepSize","text":"The 2-norm of the stepsizes that would be taken for all blocks.\n\nFor example, if there are two blocks, and we would take a stepsize of A to update one block and B to update the other, this would return sqrt(A^2 + B^2).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.FactorNorms","page":"Types","title":"BlockTensorFactorization.Core.FactorNorms","text":"FactorNorms(; norm, kwargs...)\n\nMakes a tuple containing the norm of each factor in the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GenericConstraint","page":"Types","title":"BlockTensorFactorization.Core.GenericConstraint","text":"GenericConstraint <: AbstractConstraint\n\nGeneral constraint. Simply applies the function apply and checks it was successful with check.\n\nCalling a GenericConstraint on an AbstractArray will use the function in the field apply. Use check(C::GenericConstraint, A) to use the function in the field check.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GenericDecomposition","page":"Types","title":"BlockTensorFactorization.Core.GenericDecomposition","text":"Most general decomposition. Takes the form of interweaving contractions between the factors.\n\nFor example, T = A * B + C could be represented as GenericDecomposition((A, B, C), (*, +))\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientDescent","page":"Types","title":"BlockTensorFactorization.Core.GradientDescent","text":"Perform a Gradient decent step on the nth factor of an Abstract Decomposition x\n\nThe n is only to keep track of the factor that gets updated, and to check if a frozen factor was requested to be updated.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientNNCone","page":"Types","title":"BlockTensorFactorization.Core.GradientNNCone","text":"GradientNNCone{T} <: AbstractStat\n\n2-norm vector-set distance between the negative gradient and nonnegative cone at the iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientNorm","page":"Types","title":"BlockTensorFactorization.Core.GradientNorm","text":"GradientNorm{T} <: AbstractStat\n\n2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.IterateNormDiff","page":"Types","title":"BlockTensorFactorization.Core.IterateNormDiff","text":"IterateNormDiff{T<:Function} <: AbstractStat\n\n2-norm of the difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.IterateRelativeDiff","page":"Types","title":"BlockTensorFactorization.Core.IterateRelativeDiff","text":"IterateRelativeDiff{T<:Function} <: AbstractStat\n\nRelative difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Iteration","page":"Types","title":"BlockTensorFactorization.Core.Iteration","text":"Iteration <: AbstractStat\n\nIteration number.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.L2","page":"Types","title":"BlockTensorFactorization.Core.L2","text":"L2 <: AbstractObjective\n\nThe least squares objective.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.L2-Tuple{Any, Any}","page":"Types","title":"BlockTensorFactorization.Core.L2","text":"(objective::L2)(X, Y)\n\nCalculates the least squares objective at tensors X and Y.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.LinearConstraint","page":"Types","title":"BlockTensorFactorization.Core.LinearConstraint","text":"LinearConstraint(A::T, B::AbstractArray) where {T <: Union{Function, AbstractArray}}\n\nThe constraint AX = B for a linear operator A and array B.\n\nWhen A is a matrix, this projects onto the subspace with the solution given by X .-= A' * ( (A*A') \\ (A*X .- b) )\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.LipschitzStep","page":"Types","title":"BlockTensorFactorization.Core.LipschitzStep","text":"LipschitzStep <: AbstractStep\n\nHas a single property lipschitz which stores a function for calculating the Lipschitz constant of the gradient with respect to a factor.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.LipschitzStep-Tuple{Any}","page":"Types","title":"BlockTensorFactorization.Core.LipschitzStep","text":"(step::LipschitzStep)(x; kwargs...)\n\nComputes the step size 1/L.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.MomentumUpdate-Tuple{BlockTensorFactorization.Core.AbstractGradientDescent}","page":"Types","title":"BlockTensorFactorization.Core.MomentumUpdate","text":"Makes a MomentumUpdate from an AbstractGradientDescent assuming the AbstractGradientDescent has a lipschitz step size\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.NoConstraint","page":"Types","title":"BlockTensorFactorization.Core.NoConstraint","text":"NoConstraint() <: AbstractConstraint\n\nThe constraint that does nothing. Useful for giving a list of AbstractConstraint for each factor where you would like one factor to be unconstrained.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ObjectiveRatio","page":"Types","title":"BlockTensorFactorization.Core.ObjectiveRatio","text":"ObjectiveRatio{T<:AbstractObjective} <: AbstractStat\n\nRatio between the previous and current objective value.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ObjectiveValue","page":"Types","title":"BlockTensorFactorization.Core.ObjectiveValue","text":"ObjectiveValue{T<:AbstractObjective} <: AbstractStat\n\nThe current objective value.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.PrintStats","page":"Types","title":"BlockTensorFactorization.Core.PrintStats","text":"PrintStats(; kwargs...)\n\nDoes not use any of the kwargs. Simply prints the most recent row of the stats.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ProjectedNormalization","page":"Types","title":"BlockTensorFactorization.Core.ProjectedNormalization","text":"ProjectedNormalization(projection, norm; whats_normalized=identity)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be the same size as the output of whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Projection","page":"Types","title":"BlockTensorFactorization.Core.Projection","text":"Perform a projected gradient update on the nth factor of an Abstract Decomposition x\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.RelativeError","page":"Types","title":"BlockTensorFactorization.Core.RelativeError","text":"RelativeError{T<:Function} <: AbstractStat\n\nRelative error between the decomposition model, and input array.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Rescale","page":"Types","title":"BlockTensorFactorization.Core.Rescale","text":"Rescale{T<:Union{Nothing,Missing,Function}} <: ConstraintUpdate\nRescale(n, scale::ScaledNormalization, whats_rescaled::T)\n\nApplies the scaled normalization scale to factor n, and tries to multiply the scaling of factor n to other factors.\n\nIf whats_rescaled=nothing, then it will not rescale any other factor.\n\nIf whats_rescaled=missing, then it will try to evenly distribute the weight to all other factors using the (N-1) root of each weight where N is the number of factors. If the weights are not broadcastable, (e.g. you want to scale each row but each factor has a different number of rows), will use the geometric mean of the weights as the single weight to distribute evenly among the other factors.\n\nIf typeof(whats_rescaled) <: Function, will broadcast the weight to the output of calling this function on the entire decomposition. For example,     whats_rescale = x -> eachcol(factor(x, 2)) will rescale each column of the second factor of the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ScaledNormalization","page":"Types","title":"BlockTensorFactorization.Core.ScaledNormalization","text":"ScaledNormalization(norm; whats_normalized=identity, scale=1)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be broadcast-able with the output of whats_normalized. Lastly, scale can be a Function which will act on an AbstractArray{<:Real} and return something that is broadcast-able whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.SingletonDecomposition","page":"Types","title":"BlockTensorFactorization.Core.SingletonDecomposition","text":"SingletonDecomposition(A::AbstractArray, frozen=false)\n\nWraps an AbstractArray so it can be treated like an AbstractDecomposition\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.SuperDiagonal","page":"Types","title":"BlockTensorFactorization.Core.SuperDiagonal","text":"SuperDiagonal(v::AbstractVector, ndims::Integer=2)\n\nConstructs a SuperDiagonal array from the vector v.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.SuperDiagonal-2","page":"Types","title":"BlockTensorFactorization.Core.SuperDiagonal","text":"SuperDiagonal{T, N, V<:AbstractVector{T}} <: AbstractArray{T, N}\n\nArray of order N that is zero everywhere except possibly along the super diagonal.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker-Union{Tuple{N}, Tuple{NTuple{N, Integer}, NTuple{N, Integer}}} where N","page":"Types","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker(full_size::NTuple{N, Integer}, ranks::NTuple{N, Integer};\n    frozen=false_tuple(length(ranks)+1), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker type using init to initialize the factors.\n\nSee Tucker1.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker-Union{Tuple{Tuple{Vararg{AbstractArray{T}}}}, Tuple{T}, Tuple{Tuple{Vararg{AbstractArray{T}}}, Any}} where T","page":"Types","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker((G, A, B, ...))\nTucker((G, A, B, ...), frozen)\n\nTucker decomposition. Takes the form of a core G times a matrix for each dimension.\n\nFor example, a rank (r, s, t) Tucker decomposition of an order three tensor D would be, entry-wise, D[i, j, k] = ∑r ∑s ∑_t G[r, s, t] * A[i, r] * B[j, s] * C[k, t].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee tuckerproduct.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker1-Union{Tuple{N}, Tuple{NTuple{N, Integer}, Integer}} where N","page":"Types","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1(full_size::NTuple{N, Integer}, rank::Integer; frozen=false_tuple(2), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker1 type using init to initialize the factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker1-Union{Tuple{Tuple{AbstractArray{T}, AbstractMatrix{T}}}, Tuple{T}, Tuple{Tuple{AbstractArray{T}, AbstractMatrix{T}}, Any}} where T","page":"Types","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1((G, A))\nTucker1((G, A), frozen)\n\nTucker-1 decomposition. Takes the form of a core G times a matrix A. Entry-wise\n\nD[i₁, …, iN] = ∑r G[r, i₂, …, i_N] * A[i₁, r].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee ×₁ and mtt.\n\n\n\n\n\n","category":"method"},{"location":"#Block-Tensor-Decomposition","page":"Home","title":"Block Tensor Decomposition","text":"","category":"section"},{"location":"#About-this-package","page":"Home","title":"About this package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockTensorFactorization.jl is a package to factorize tensors. The main feature is its flexibility at decomposing input tensors according to many common tensor models (ex. CP, Tucker) with a number of constraints (ex. nonnegative, simplex).","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Coming Soon) The package also supports user defined models and constraints provided the operations for combining factor into a tensor, and projecting/applying the constraint are given. It is also a longer term goal to support other optimization objective beyond minimizing the least-squares (Frobenius norm) between the input tensor and model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The general scheme for computing the decomposition is a generalization of Xu and Yin's Block Coordinate Descent Method (2013) that cyclicaly updates each factor in a model with a proximal gradient descent step. Note for convex constraints, the proximal operation would be a Euclidian projection onto the constraint set, but we find some improvment with a hybrid approach of a partial Euclidian projection followed by a rescaling step. In the case of a simplex constraint on one factor, this looks like: dividing the constrained factor by the sum of entries, and multiplying another factor by this sum to preserve the product.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depth = 3","category":"page"}]
}

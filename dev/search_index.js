var documenterSearchIndex = {"docs":
[{"location":"quickguide/#Quick-Guide","page":"Quick Guide","title":"Quick Guide","text":"","category":"section"},{"location":"quickguide/#Factorizing-your-data","page":"Quick Guide","title":"Factorizing your data","text":"The main feature of this package is to factorize an array. This is accomplished with the factorize function.\n\nSay you've collected your data into an order-3 tensor Y. You can use randn from Random.jl to simulate this.\n\nusing Random\n\nY = randn(100,100,100)\n\nThen you can call factorize with a number of keywords. The main keywords you many want to specify are the model and rank. This lets factorize know they type and size of the decomposition. See Decomposition Models for a complete list of avalible models, and how to define your own custom decomposition.\n\nusing BlockTensorFactorization\n\nX, stats, kwargs = factorize(Y; model=Tucker1, rank=5)","category":"section"},{"location":"quickguide/#Extracting-factors","page":"Quick Guide","title":"Extracting factors","text":"The main output, X, is of type model (Tucker1). We can call factors to see what the decomposed factors are. Or you can call factor(X,n) to just extract the nth factor. This this case, there are only two factors; the order-3 core (0th factor) G and the matrix (1st factor) A. The following would all be valid ways to extract these factors.\n\ntypeof(X) == Tucker1{Float64, 3}\n\n# Works on any model\nG, A = factors(X)\nG = factor(X, 0)\nA = factor(X, 1)\n\n# Exclusive to Tucker1, Tucker, and CPDecomposition\nG = core(X)\nA = matrix_factors(X)[begin]\n\nsize(G) == (5, 100, 100)\nsize(A) == (100, 5)\nsize(X) == (100, 100, 100)\n\nSince the models are subtypes of AbstractArray, all the usual array operations can be performed directly on the decomposition.\n\n# Difference between a Tucker1 type and a regular Array type\nX - Y\n\n# Entry X_{123} in the tensor\nX[1, 2, 3]\n\nIf you every want to \"flatten\" the model into a regular Array,  you can call array, or multiply the factors yourself.\n\nZ = array(X)\ntypeof(Z) == Array{Float64, 3}\n\nG ×₁ A == Z # 1-mode product\nnmode_product(G, A, 1) == Z # 1-mode product\nmtt(A, G) == Z # matrix times tensor","category":"section"},{"location":"quickguide/#Iteration-Statistics","page":"Quick Guide","title":"Iteration Statistics","text":"The output variable stats is a DataFrame that records the requested stats every iteration. You can pass a list of supported stats, or custom stats. See Iteration Stats for more details.\n\nBy default, the iteration number, objective value (L2 norm between the input and the model in this case), and the Euclidean norm of the gradient (of the loss function at the current iteration) are recorded. The following would reproduce the default stats in our running example.\n\nX, stats, kwargs = factorize(Y;\n    stats=[Iteration, ObjectiveValue, GradientNorm]\n)\n\nThe full results can be displayed in the REPL, or a vector of the individual stat at some or every iteration can be extracted by using the Symbol of that stat (prepend a colon : to the name).\n\ndisplay(stats) # Display the full DataFrame\n\nstats[end, :ObjectiveValue] # Final objective value\nstats[:, :ObjectiveValue] # Objective value at every iteration\n\nYou may also want to see every stat at a particular iteration which can be accessed in the following way. Note that the initilization is stored in the first row, so the nth row stores the stats right before the nth iteration, not after.\n\nstats[begin, :] # Every stat at the initialization\nstats[4, :] # Every stat right *before* the 4th iteration\nstats[end, :] # Every stat at the final iteration\n\nSee the DataFrames.jl package for more data handeling.","category":"section"},{"location":"quickguide/#Output-keyword-arguments","page":"Quick Guide","title":"Output keyword arguments","text":"Since there are many options and a complicated handeling of defaults arguments, the factorize function also outputs all the keyword arguments as a NamedTuple. This allows you to check what keywords you set, along with the default values that were substituted for the keywords you did not provide.\n\nYou can access the values by getting the relevent field, or index (as a Symbol). In our running example, this would look like the following.\n\nkwargs.rank == 5\nkwargs[:rank] == 5\ngetfield(kwargs, :rank) == 5\n\nkwarks.model == Tucker1\nkwargs[:model] == Tucker1\ngetfield(kwargs, :model) == Tucker1","category":"section"},{"location":"reference/#Index","page":"Index","title":"Index","text":"The following is a complete list of exported terms in this package.\n\n","category":"section"},{"location":"tutorial/multiscale/#Multiscale-Factorization","page":"Multiscale Factorization","title":"Multiscale Factorization","text":"","category":"section"},{"location":"tutorial/multiscale/#Quick-Start","page":"Multiscale Factorization","title":"Quick Start","text":"In some applications, the input tensor Y to be factorized is a discretization of continuous functions. For example,\n\nYijk = f_i(x_j y_k)\n\nfor continuous functions f_i and (ijk)inItimesJtimesK. Instead of calling factorize\n\nfactorize(Y; kwargs..)\n\nwe can use multiscale_factorize, and pass along information about which dimension come from continuously varying values.\n\nmultiscale_factorize(Y; continuous_dims=(2,3), kwargs...)\n\nFor fine discretizations, this is often faster and less memory intensive.","category":"section"},{"location":"tutorial/multiscale/#Motivating-example","page":"Multiscale Factorization","title":"Motivating example","text":"Say we have an order 3-tensor Y with entries\n\nYi j k = xi yj zk\n\nfor (ijk) in 65times129times257 representing a discretization of the box -1 1times 0 10times 01.\n\nWe might construct this in Julia with the following code.\n\nf(x, y, z) = x*y*z\nx = reshape(range(-1, 1, length=65), 65, 1, 1)\ny = reshape(range(0, 10, length=129), 1, 129, 1)\nz = reshape(range(0, 1, length=257), 1, 1, 257)\nY = f.(x, y, z)\n\nIt is true that Y is a CP-rank-1 tensor. So we could recover the factors x, y, and z with\n\ndecomposition, stats, kwargs = multiscale_factorize(Y;\n    model=CPDecomposition, rank=1, continuous_dims=(1,2,3))\nx, y, z = factors(decomposition)\n\nThe grid (ijk) in 65times129times257 we chose to discretize the function f(xyz)=xyz was arbitrary and we could just as easily discretized f on a smaller grid (ijk)in10^3 or larger one (ijk)in257^3.\n\nIn this setting, we can first decompose a coarse version of Y, for example only using every second grid-points in each dimension\n\nY_coarse = Y[begin:2:end, begin:2:end, begin:2:end]\n\nand compute a coarse decomposition\n\nmodel = CPDecomposition\nrank = 1\ndecomposition_coarse, _, _ = factorize(Y_coarse; model, rank)\nx_coarse, y_coarse, z_coarse = factors(decomposition_coarse)\n\nWe could then initialize the fine scale factorization using an interpolation of the coarse factors found.\n\ninit = interpolate.(factors(decomposition))\nfactorize(Y; model, rank, init)\n\nWe can actually do this starting from the most coarse scale with only 3 points, and gradually refine the decomposition over multiple scale. This is what multiscale_factorize does.","category":"section"},{"location":"tutorial/multiscale/#Related-exported-functions","page":"Multiscale Factorization","title":"Related exported functions","text":"","category":"section"},{"location":"tutorial/multiscale/#BlockTensorFactorization.Core.multiscale_factorize-tutorial-multiscale","page":"Multiscale Factorization","title":"BlockTensorFactorization.Core.multiscale_factorize","text":"multiscale_factorize(Y; continuous_dims=1:ndims(Y), rank=1, model=Tucker1, kwargs...)\n\nLike factorize but uses progressively finer sub-grids of Y to speed up convergence. This is only effective when the dimensions given by dims come from discretizations of continuous data.\n\nFor example, if Y has 3 dimensions where Y[i, j, k] are samples from a continuous 2D function fi(xj, y_k) on a grid, use\n\nmultiscale_factorize(Y; continuous_dims=(2,3))\n\nsince second and third dimensions are continuous.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/multiscale/#BlockTensorFactorization.Core.coarsen-tutorial-multiscale","page":"Multiscale Factorization","title":"BlockTensorFactorization.Core.coarsen","text":"coarsen(Y::AbstractArray, scale::Integer; dims=1:ndims(Y))\n\nCoarsens or downsamples Y by scale. Only keeps every scale entries along the dimensions specified.\n\nExample\n\nY = randn(12, 12, 12)\n\ncoarsen(Y, 2) == Y[begin:2:end, begin:2:end, begin:2:end]\n\ncoarsen(Y, 4; dims=(1, 3)) == Y[begin:4:end, :, begin:4:end]\n\ncoarsen(Y, 3; dims=2) == Y[:, begin:3:end, :]\n\n\n\n\n\n","category":"function"},{"location":"tutorial/multiscale/#BlockTensorFactorization.Core.interpolate-tutorial-multiscale","page":"Multiscale Factorization","title":"BlockTensorFactorization.Core.interpolate","text":"interpolate(Y, scale; dims=1:ndims(Y), degree=0, kwargs...)\n\nInterpolates Y to a larger array with repeated values.\n\nKeywords\n\nscale. How much to scale up the size of Y. A dimension with size k will be scaled to scale*k - (scale - 1) = scale*(k-1) + 1\n\ndims:1:ndims(Y). Which dimensions to interpolate.\n\ndegree:0. What degree of interpolation to use. 0 is constant interpolation, 1 is linear.\n\nLike the opposite of coarsen.\n\nExample\n\njulia> Y = collect(reshape(1:6, 2, 3))\n2×3 Matrix{Int64}:\n 1  3  5\n 2  4  6\n\njulia> interpolate(Y, 2)\n3×5 Matrix{Int64}:\n 1  1  3  3  5\n 1  1  3  3  5\n 2  2  4  4  6\n\njulia> interpolate(Y, 3; dims=2)\n2×7 Matrix{Int64}:\n 1  1  1  3  3  3  5\n 2  2  2  4  4  4  6\n\njulia> interpolate(Y, 1) == Y\ntrue\n\n\n\n\n\n","category":"function"},{"location":"tutorial/multiscale/#BlockTensorFactorization.Core.scale_constraint-tutorial-multiscale","page":"Multiscale Factorization","title":"BlockTensorFactorization.Core.scale_constraint","text":"scale_constraint(constraint::AbstractConstraint, scale, n_continuous_dims)\n\nReturns a scaled version of the constraint based off the number of relevant continuous dimensions the constraint acts on.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#Constrained-Factorization","page":"Constrained Factorization","title":"Constrained Factorization","text":"","category":"section"},{"location":"tutorial/constraints/#Example","page":"Constrained Factorization","title":"Example","text":"A number of constraints are ready-to-use out of the box. Say you would like to perform a rank 5 CP decomposition of a 3rd order tensor, such that every column is constrainted to the simplex. This can be accomplished by the following code.\n\nX, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5, constraints=simplex_cols!)\n\nOnly one constraint is given when there are 3 factors in the CP decomposition of Y, so it is assumed this constraint applies to every factor.\n\nSay you instead you only want the 1st factor to have columns constrained to the simplex, and the 3rd factor to be nonnegative. You can use the following code.\n\nX, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5,\n    constraints=[simplex_cols!, noconstraint, nonnegative!])","category":"section"},{"location":"tutorial/constraints/#Ready-to-use-Constraints","page":"Constrained Factorization","title":"Ready-to-use Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#Entrywise","page":"Constrained Factorization","title":"Entrywise","text":"The constraints nonnegative! and binary! ensure each entry in a factor is nonnegative and either 0 or 1 respectively.\n\nIf you want every entry to be in the closed interval a, b, you can use IntervalConstraint(a, b).","category":"section"},{"location":"tutorial/constraints/#Normalizations","page":"Constrained Factorization","title":"Normalizations","text":"These ensure a factor is normalized according to the L1, L2, & L infinity norms. This is accomplished through a Euclidean projections onto the unit ball.\n\nEach come in versions that constrain each row, column, order-1 slice, & order-(1, 2) slice to the associated unit norm ball.\n\nl1normalize_rows!\nl1normalize_cols!\nl1normalize_1slices!\nl1normalize_12slices!\n\nl2normalize_rows!\nl2normalize_cols!\nl2normalize_1slices!\nl2normalize_12slices!\n\nlinftynormalize_rows!\nlinftynormalize_cols!\nlinftynormalize_1slices!\nlinftynormalize_12slices!\n\nwarning: Warning\nProjection onto the unit norm ball from the origin is not unique at the origin so do not expect consistent behaviour with an all-zeros input.","category":"section"},{"location":"tutorial/constraints/#Scaling-Constraints","page":"Constrained Factorization","title":"Scaling Constraints","text":"Each previously listed normalization has an associated scaled normalization. These ensure the relevant subarrays are normalized, but rather than enforce these by a Euclidean projection, they simply divide by its norm. This is equivalent to the Euclidean projection onto the L2 norm ball, but is a different operation for the L1 and L infinity balls. This offers the advantage that other factors can be \"rescaled\" to compensate for this division which is not normally possible with the projections onto the L1 and L infinity balls.\n\nnote: Note\nBy default, when these constraints are applied, they will \"rescale\" the other factors to minimize the change in the product of all the factors.\n\ndetails: Example\nSay we are performing CPDecomposition on a matrix Y. This is equivalent to factorizing Y = A * B'. If we would like all columns of B (rows of B transpose) to be on the L1 ball, rather than projecting each column, we can instead divide each column of B by its L1 norm, and multiply the associated column of A by this amount. This has the advantage of enforcing our constraint without effecting the product A * B', whereas a projection would possibly change this product.\n\nl1scale!\nl1scale_rows!\nl1scale_cols!\nl1scale_1slices!\nl1scale_12slices!\n\nl2scale!\nl2scale_rows!\nl2scale_cols!\nl2scale_1slices!\nl2scale_12slices!\n\nlinftyscale!\nlinftyscale_rows!\nlinftyscale_cols!\nlinftyscale_1slices!\nlinftyscale_12slices!\n\nThere is also a set of constraints that ensure the order-(1,2) slices are scaled on average. This makes preserving a Tucker1 product possible where you would like each order-(1,2) normalized.\n\nl1scale_average12slices!\nl2scale_average12slices!\nlinftyscale_average12slices!","category":"section"},{"location":"tutorial/constraints/#Simplex-Constraint","page":"Constrained Factorization","title":"Simplex Constraint","text":"Similar to the L1 normalization constraint, these constraints ensure the relevant subarrays are on the L1 ball. But these also ensure all entries are positive. This is enforced with a single Euclidean projection onto the relevant simplex.\n\nsimplex!\nsimplex_cols!\nsimplex_1slices!\nsimplex_12slices!","category":"section"},{"location":"tutorial/constraints/#List-of-built-in-ProjectedNormalizations-and-ScaledNormalizations","page":"Constrained Factorization","title":"List of built-in ProjectedNormalizations and ScaledNormalizations","text":"l1normalize!\nl1normalize_rows!\nl1normalize_cols!\nl1normalize_1slices!\nl1normalize_12slices!\nl2normalize!\nl2normalize_rows!\nl2normalize_cols!\nl2normalize_1slices!\nl2normalize_12slices!\nlinftynormalize!\nlinftynormalize_rows!\nlinftynormalize_cols!\nlinftynormalize_1slices!\nlinftynormalize_12slices!\nsimplex!\nsimplex_rows!\nsimplex_cols!\nsimplex_1slices!\nsimplex_12slices!\nl1scale!\nl1scale_rows!\nl1scale_cols!\nl1scale_1slices!\nl1scale_12slices!\nl1scale_average12slices!\nl2scale!\nl2scale_rows!\nl2scale_cols!\nl2scale_1slices!\nl2scale_12slices!\nl2scale_average12slices!\nlinftyscale!\nlinftyscale_rows!\nlinftyscale_cols!\nlinftyscale_1slices!\nlinftyscale_12slices!\nlinftyscale_average12slices!","category":"section"},{"location":"tutorial/constraints/#Linear-Constraints","page":"Constrained Factorization","title":"Linear Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#Advanced-Constraints","page":"Constrained Factorization","title":"Advanced Constraints","text":"","category":"section"},{"location":"tutorial/constraints/#Ending-with-a-different-set-of-constraints","page":"Constrained Factorization","title":"Ending with a different set of constraints","text":"It is possible to apply a different set of constraints at the end of the algorithm than what is enforced during the iterations. For example, you can perform nonnegative Tucker factorization of a tensor Y, but apply a simplex constraint on the core at the very end.\n\nX, stats, kwargs = factorize(Y; model=Tucker, rank=2,\n    constraints=nonnegative!,\n    final_constraints=[simplex!, noconstraint, noconstraint, noconstraint])\n\nIn the case where constraints effect other factors (e.g. l2scale!), you may want to perform a final pass of the constraints to ensure each factor is scaled correctly, without rescaling/effecting other factors.\n\nX, stats, kwargs = factorize(Y; model=CPDecomposition, rank=3,\n    constraints=l2scale_cols!,\n    constrain_output=true)","category":"section"},{"location":"tutorial/constraints/#Composing-Constraints","page":"Constrained Factorization","title":"Composing Constraints","text":"AbstractConstraint types can be composed with \\circ (and hitting tab to make ∘) creating a ComposedConstraint.\n\nwarning: Warning\nAll ComposedConstraints do is apply the two constraints in series and does not do anything intelligent like finding the intersection of the constraints. For this reason, the following three constraints are all different.l1normalize! ∘ nonnegative!\nnonnegative! ∘ l1normalize!\nsimplex!","category":"section"},{"location":"tutorial/constraints/#Custom-Constraints","page":"Constrained Factorization","title":"Custom Constraints","text":"You can define your own ProjectedNormalization, ScaledNormalization, or Entrywise constraint using the following constructors.\n\nScaledNormalization(norm; whats_normalized=identityslice, scale=1)\nProjectedNormalization(norm, projection; whats_normalized=identityslice)\nEntrywise(apply, check)\n\nIn fact, these are how the ready-to-use constraints are made. Here are some examples.\n\nl2scale_1slices! = ScaledNormalization(l2norm; whats_normalized=(x -> eachslice(x; dims=1)))\nl1normalize_rows! = ProjectedNormalization(l1norm, l1project!; whats_normalized=eachrow)\nnonnegative! = Entrywise(ReLU, isnonnegative)\nIntervalConstraint(a, b) = Entrywise(x -> clamp(x, a, b), x -> a <= x <= b)\n\nYou can also make a custom constraint with GenericConstraint.","category":"section"},{"location":"tutorial/constraints/#Manual-Constraint-Updates","page":"Constrained Factorization","title":"Manual Constraint Updates","text":"You can manually define the ConstraintUpdate that gets applied as part of the block decomposition method. These will be automatically inserted into the order of updates immediately following the last update of the matching block with smart_interlace!.\n\nAs an example, if we are performing CP decomposition on an order 3 tensor, the unconstrained block optimization would look something like this.\n\nBlockUpdate(\n    GradientDescent(1, gradient, step)\n    GradientDescent(2, gradient, step)\n    GradientDescent(3, gradient, step)\n)\n\nHere the 1, 2, and 3 denote which factor gets updated. If we want to apply a simplex constraint to the second factor, and nonnegative to the 3rd, you can do the following.\n\nX, stats, kwargs = factorize(Y; model=CPDecomposition, rank=5, constraints=[ConstraintUpdate(3, nonnegative!), Projection(2, simplex!)])\n\nThis would result in the following block update.\n\nBlockUpdate(\n    GradientDescent(1, gradient, step)\n    GradientDescent(2, gradient, step)\n    Projection(2, simplex!)\n    GradientDescent(3, gradient, step)\n    Projection(3, nonnegative!)\n)\n\nNote the order [ConstraintUpdate(3, nonnegative!), Projection(2, simplex!)] does not matter. Also, using the (abstract) constructor ConstraintUpdate will reduce to the concrete types when possible.\n\nWhen using ScaledNormalizations, you may want to manually define what gets rescaled using the downstream ConstraintUpdate concrete type: Rescale.","category":"section"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.Entrywise-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.Entrywise","text":"Entrywise <: AbstractConstraint\nEntrywise(apply::Function, check::Function)\n\nEntry-wise constraint. Both apply and check need are performed entry-wise on an array.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.nonnegative!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.nonnegative!","text":"Entrywise(ReLU, isnonnegative)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.binary!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.binary!","text":"Entrywise(binaryproject, x -> x in (0, 1))\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.IntervalConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.IntervalConstraint","text":"Entrywise(x -> clamp(x, a, b), x -> a ≤ x ≤ b)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ProjectedNormalization-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ProjectedNormalization","text":"ProjectedNormalization(projection, norm; whats_normalized=identity)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be the same size as the output of whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.l1normalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.l1normalize!","text":"l1normalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.l2normalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.l2normalize!","text":"l2normalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.linftynormalize!-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.linftynormalize!","text":"linftynormalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ScaledNormalization-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ScaledNormalization","text":"ScaledNormalization(norm; whats_normalized=identity, scale=1)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be broadcast-able with the output of whats_normalized. Lastly, scale can be a Function which will act on an AbstractArray{<:Real} and return something that is broadcast-able whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.BUILT_IN_CONSTRAINTS","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.BUILT_IN_CONSTRAINTS","text":"List of symbols of the built-in constraint functions.\n\n\n\n\n\n","category":"constant"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.LinearConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.LinearConstraint","text":"LinearConstraint(A::T, B::AbstractArray) where {T <: Union{Function, AbstractArray}}\n\nThe constraint AX = B for a linear operator A and array B.\n\nWhen A is a matrix, this projects onto the subspace with the solution given by\n\nX .-= A' * ( (A*A') \\ (A*X .- b) ).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ComposedConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ComposedConstraint","text":"ComposedConstraint{T<:AbstractConstraint, U<:AbstractConstraint}\nouter_constraint ∘ inner_constraint\n\nComposing any two AbstractConstraints with ∘ will return this type.\n\nApplies the inner constraint first, then the outer constraint. Checking a ComposedConstraint will check both constraints are satisfied.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.GenericConstraint-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.GenericConstraint","text":"GenericConstraint <: AbstractConstraint\n\nGeneral constraint. Simply applies the function apply and checks it was successful with check.\n\nCalling a GenericConstraint on an AbstractArray will use the function in the field apply. Use check(C::GenericConstraint, A) to use the function in the field check.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.ConstraintUpdate-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.ConstraintUpdate","text":"ConstraintUpdate(n, constraint)\n\nConverts an AbstractConstraint to a ConstraintUpdate on the factor n\n\n\n\n\n\n","category":"type"},{"location":"tutorial/constraints/#BlockTensorFactorization.Core.Rescale-tutorial-constraints","page":"Constrained Factorization","title":"BlockTensorFactorization.Core.Rescale","text":"Rescale{T<:Union{Nothing,Missing,Function}} <: ConstraintUpdate\nRescale(n, scale::ScaledNormalization, whats_rescaled::T)\n\nApplies the scaled normalization scale to factor n, and tries to multiply the scaling of factor n to other factors.\n\nIf whats_rescaled=nothing, then it will not rescale any other factor.\n\nIf whats_rescaled=missing, then it will try to evenly distribute the weight to all other factors using the (N-1) root of each weight where N is the number of factors. If the weights are not broadcastable, (e.g. you want to scale each row but each factor has a different number of rows), will use the geometric mean of the weights as the single weight to distribute evenly among the other factors.\n\nIf typeof(whats_rescaled) <: Function, will broadcast the weight to the output of calling this function on the entire decomposition. For example,     whats_rescale = x -> eachcol(factor(x, 2)) will rescale each column of the second factor of the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#Decomposition-Models","page":"Decomposition Models","title":"Decomposition Models","text":"The main abstract type for all decomposition models is AbstractDecomposition.\n\nThis can be subtyped to create your own decomposition model.\n\nThe following common tensor models are available as valid arguments to model and built into this package.","category":"section"},{"location":"tutorial/decompositionmodels/#Tucker-types","page":"Decomposition Models","title":"Tucker types","text":"Note these are all subtypes of AbstractTucker.","category":"section"},{"location":"tutorial/decompositionmodels/#Other-types","page":"Decomposition Models","title":"Other types","text":"","category":"section"},{"location":"tutorial/decompositionmodels/#How-Julia-treats-an-AbstractDecomposition","page":"Decomposition Models","title":"How Julia treats an AbstractDecomposition","text":"AbstractDecomposition is an abstract subtype of AbstractArray. AbstractDecomposition will keep track of the element type and number of dimensions like other AbstractArray. This is the T and N in the type Array{T,N}. To make AbstractDecomposition behave like other array types, Julia only needs to know how to access/compute indexes of the array through getindex. These indices are computed on the fly when a particular index is requested, or the whole tensor is computed from its factors through array(X). This has the advantage of minimizing the memory used, and allows for the most flexibility since any operation that is supported by AbstractArray will work on AbstractDecomposition types. The drawback is that repeated requests to entries must recompute the entry each time. In these cases, it is best to \"flatten\" the array with array(X) first before making these repeated calls.\n\nSome basic operations like +, -, *, \\, and / will either compute the operation is some optimized way, or call array(X) function to first flatten the decomposition into a regular Array type in some optimized way. Operations that don't have an optimized method (because I can only do so much), will instead call Julia's Array{T,N}(X) to convert the model into a regular Array{T,N} type. This is usually slower and less memory efficient since it calls getindex on every index individually, instead of computing the whole array at once.","category":"section"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.AbstractDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.AbstractDecomposition","text":"Abstract type for the different decompositions.\n\nMain interface for subtypes are the following functions. Required:     array(D): how to construct the full array from the decomposition representation     factors(D): a tuple of arrays, the decomposed factors Optional:     getindex(D, i::Int) and getindex(D, I::Vararg{Int, N}): how to get the ith or Ith         element of the reconstructed array. Defaults to getindex(array(D), x), but there         is often a more efficient way to get a specific element from large tensors.     size(D): Defaults to size(array(D)).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.Tucker-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker((G, A, B, ...))\nTucker((G, A, B, ...), frozen)\n\nTucker decomposition. Takes the form of a core G times a matrix for each dimension.\n\nFor example, a rank (r, s, t) Tucker decomposition of an order three tensor D would be, entry-wise,\n\nD[i, j, k] = ∑_r ∑_s ∑_t G[r, s, t] * A[i, r] * B[j, s] * C[k, t].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee tuckerproduct.\n\n\n\n\n\nTucker(full_size::NTuple{N, Integer}, ranks::NTuple{N, Integer};\n    frozen=false_tuple(length(ranks)+1), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker type using init to initialize the factors.\n\nSee Tucker1.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.Tucker1-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1((G, A))\nTucker1((G, A), frozen)\n\nTucker-1 decomposition. Takes the form of a core G times a matrix A. Entry-wise\n\nD[i₁, …, i_N] = ∑_r G[r, i₂, …, i_N] * A[i₁, r].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee ×₁ and mtt.\n\n\n\n\n\nTucker1(full_size::NTuple{N, Integer}, rank::Integer; frozen=false_tuple(2), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker1 type using init to initialize the factors.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.CPDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.CPDecomposition","text":"CP decomposition. Takes the form of an outerproduct of multiple matrices.\n\nFor example, a CP-decomposition of an order three tensor D would be, entry-wise,\n\nD[i, j, k] = ∑_r A[i, r] * B[j, r] * C[k, r]).\n\nCPDecomposition((A, B, C))\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.AbstractTucker-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.AbstractTucker","text":"Abstract type for all Tucker-like decompositions. AbstractTucker decompositions have a core with the same number of dimensions as the full array, and (a) matrix factor(s).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.GenericDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.GenericDecomposition","text":"Most general decomposition. Takes the form of interweaving contractions between the factors.\n\nFor example, T = A * B + C could be represented as GenericDecomposition((A, B, C), (*, +))\n\n\n\n\n\n","category":"type"},{"location":"tutorial/decompositionmodels/#BlockTensorFactorization.Core.SingletonDecomposition-tutorial-decompositionmodels","page":"Decomposition Models","title":"BlockTensorFactorization.Core.SingletonDecomposition","text":"SingletonDecomposition(A::AbstractArray, frozen=false)\n\nWraps an AbstractArray so it can be treated like an AbstractDecomposition\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractConstraint","page":"Types","title":"BlockTensorFactorization.Core.AbstractConstraint","text":"Abstract parent type for the various constraints\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractDecomposition","page":"Types","title":"BlockTensorFactorization.Core.AbstractDecomposition","text":"Abstract type for the different decompositions.\n\nMain interface for subtypes are the following functions. Required:     array(D): how to construct the full array from the decomposition representation     factors(D): a tuple of arrays, the decomposed factors Optional:     getindex(D, i::Int) and getindex(D, I::Vararg{Int, N}): how to get the ith or Ith         element of the reconstructed array. Defaults to getindex(array(D), x), but there         is often a more efficient way to get a specific element from large tensors.     size(D): Defaults to size(array(D)).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractObjective","page":"Types","title":"BlockTensorFactorization.Core.AbstractObjective","text":"AbstractObjective <: Function\n\nGeneral interface is\n\nstruct L2 <: AbstractObjective end\n\nafter constructing\n\nmyobjective = L2()\n\nyou can call\n\nmyobjective(X, Y)\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractStat","page":"Types","title":"BlockTensorFactorization.Core.AbstractStat","text":"An AbstractStat is a type which, when created, can be applied to the four arguments (X::AbstractDecomposition, Y::AbstractArray, previous::Vector{<:AbstractDecomposition}, parameters::Dict) to (usually) return a number.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractStep","page":"Types","title":"BlockTensorFactorization.Core.AbstractStep","text":"Interface to make a step scheme is\n\nstruct MyStep <: AbstractStep     ... end\n\nfunction (step::MyStep)(x::AbstractDecomposition; kwargs...)     ...     return step::Real end\n\nTo use your scheme, construct an instance with any necessary parameters\n\nmystep = MyStep(...)\n\nand then you can call\n\nstep = mystep(D; kwargs...)\n\nto compute the step size.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.AbstractTucker","page":"Types","title":"BlockTensorFactorization.Core.AbstractTucker","text":"Abstract type for all Tucker-like decompositions. AbstractTucker decompositions have a core with the same number of dimensions as the full array, and (a) matrix factor(s).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.BlockGradientDescent","page":"Types","title":"BlockTensorFactorization.Core.BlockGradientDescent","text":"Perform a Block Gradient decent step on the nth factor of an Abstract Decomposition x\n\nThe n is only to keep track of the factor that gets updated, and to check if a frozen factor was requested to be updated.\n\nThis type allows for more complicated step sizes such as individual steps for sub-blocks of the nth factor.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.CPDecomposition","page":"Types","title":"BlockTensorFactorization.Core.CPDecomposition","text":"CP decomposition. Takes the form of an outerproduct of multiple matrices.\n\nFor example, a CP-decomposition of an order three tensor D would be, entry-wise,\n\nD[i, j, k] = ∑_r A[i, r] * B[j, r] * C[k, r]).\n\nCPDecomposition((A, B, C))\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ComposedConstraint","page":"Types","title":"BlockTensorFactorization.Core.ComposedConstraint","text":"ComposedConstraint{T<:AbstractConstraint, U<:AbstractConstraint}\nouter_constraint ∘ inner_constraint\n\nComposing any two AbstractConstraints with ∘ will return this type.\n\nApplies the inner constraint first, then the outer constraint. Checking a ComposedConstraint will check both constraints are satisfied.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ConstraintUpdate-Tuple{Any, AbstractConstraint}","page":"Types","title":"BlockTensorFactorization.Core.ConstraintUpdate","text":"ConstraintUpdate(n, constraint)\n\nConverts an AbstractConstraint to a ConstraintUpdate on the factor n\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.DiagonalTuple","page":"Types","title":"BlockTensorFactorization.Core.DiagonalTuple","text":"Alias for NTuple{N, Diagonal{T}}\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.DisplayDecomposition","page":"Types","title":"BlockTensorFactorization.Core.DisplayDecomposition","text":"DisplayDecomposition(; kwargs...)\n\nDoes not use any of the kwargs. Simply displays the current iteration.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Entrywise","page":"Types","title":"BlockTensorFactorization.Core.Entrywise","text":"Entrywise <: AbstractConstraint\nEntrywise(apply::Function, check::Function)\n\nEntry-wise constraint. Both apply and check need are performed entry-wise on an array.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Entrywise-Tuple{AbstractArray}","page":"Types","title":"BlockTensorFactorization.Core.Entrywise","text":"(C::Entrywise)(A::AbstractArray)\n\nApplies C.apply to A entry-wise. Mutates A.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.EuclideanLipschitz","page":"Types","title":"BlockTensorFactorization.Core.EuclideanLipschitz","text":"The 2-norm of the lipschitz constants that would be taken for all blocks.\n\nNeed the stepsizes to be lipschitz steps since it is calculated similarly to EuclideanStepSize.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.EuclideanStepSize","page":"Types","title":"BlockTensorFactorization.Core.EuclideanStepSize","text":"The 2-norm of the stepsizes that would be taken for all blocks.\n\nFor example, if there are two blocks, and we would take a stepsize of A to update one block and B to update the other, this would return sqrt(A^2 + B^2).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.FactorNorms","page":"Types","title":"BlockTensorFactorization.Core.FactorNorms","text":"FactorNorms(; norm, kwargs...)\n\nMakes a tuple containing the norm of each factor in the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GenericConstraint","page":"Types","title":"BlockTensorFactorization.Core.GenericConstraint","text":"GenericConstraint <: AbstractConstraint\n\nGeneral constraint. Simply applies the function apply and checks it was successful with check.\n\nCalling a GenericConstraint on an AbstractArray will use the function in the field apply. Use check(C::GenericConstraint, A) to use the function in the field check.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GenericDecomposition","page":"Types","title":"BlockTensorFactorization.Core.GenericDecomposition","text":"Most general decomposition. Takes the form of interweaving contractions between the factors.\n\nFor example, T = A * B + C could be represented as GenericDecomposition((A, B, C), (*, +))\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientDescent","page":"Types","title":"BlockTensorFactorization.Core.GradientDescent","text":"Perform a Gradient decent step on the nth factor of an Abstract Decomposition x\n\nThe n is only to keep track of the factor that gets updated, and to check if a frozen factor was requested to be updated.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientNNCone","page":"Types","title":"BlockTensorFactorization.Core.GradientNNCone","text":"GradientNNCone{T} <: AbstractStat\n\n2-norm vector-set distance between the negative gradient and nonnegative cone at the iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.GradientNorm","page":"Types","title":"BlockTensorFactorization.Core.GradientNorm","text":"GradientNorm{T} <: AbstractStat\n\n2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.IterateNormDiff","page":"Types","title":"BlockTensorFactorization.Core.IterateNormDiff","text":"IterateNormDiff{T<:Function} <: AbstractStat\n\n2-norm of the difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.IterateRelativeDiff","page":"Types","title":"BlockTensorFactorization.Core.IterateRelativeDiff","text":"IterateRelativeDiff{T<:Function} <: AbstractStat\n\nRelative difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Iteration","page":"Types","title":"BlockTensorFactorization.Core.Iteration","text":"Iteration <: AbstractStat\n\nIteration number.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.L2","page":"Types","title":"BlockTensorFactorization.Core.L2","text":"L2 <: AbstractObjective\n\nThe least squares objective.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.L2-Tuple{Any, Any}","page":"Types","title":"BlockTensorFactorization.Core.L2","text":"(objective::L2)(X, Y)\n\nCalculates the least squares objective at tensors X and Y.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.LinearConstraint","page":"Types","title":"BlockTensorFactorization.Core.LinearConstraint","text":"LinearConstraint(A::T, B::AbstractArray) where {T <: Union{Function, AbstractArray}}\n\nThe constraint AX = B for a linear operator A and array B.\n\nWhen A is a matrix, this projects onto the subspace with the solution given by\n\nX .-= A' * ( (A*A') \\ (A*X .- b) ).\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.LipschitzStep","page":"Types","title":"BlockTensorFactorization.Core.LipschitzStep","text":"LipschitzStep <: AbstractStep\n\nHas a single property lipschitz which stores a function for calculating the Lipschitz constant of the gradient with respect to a factor.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.LipschitzStep-Tuple{Any}","page":"Types","title":"BlockTensorFactorization.Core.LipschitzStep","text":"(step::LipschitzStep)(x; kwargs...)\n\nComputes the step size 1/L.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.MomentumUpdate-Tuple{BlockTensorFactorization.Core.AbstractGradientDescent}","page":"Types","title":"BlockTensorFactorization.Core.MomentumUpdate","text":"Makes a MomentumUpdate from an AbstractGradientDescent assuming the AbstractGradientDescent has a lipschitz step size\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.NoConstraint","page":"Types","title":"BlockTensorFactorization.Core.NoConstraint","text":"NoConstraint() <: AbstractConstraint\n\nThe constraint that does nothing. Useful for giving a list of AbstractConstraint for each factor where you would like one factor to be unconstrained.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ObjectiveRatio","page":"Types","title":"BlockTensorFactorization.Core.ObjectiveRatio","text":"ObjectiveRatio{T<:AbstractObjective} <: AbstractStat\n\nRatio between the previous and current objective value.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ObjectiveValue","page":"Types","title":"BlockTensorFactorization.Core.ObjectiveValue","text":"ObjectiveValue{T<:AbstractObjective} <: AbstractStat\n\nThe current objective value.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.PrintStats","page":"Types","title":"BlockTensorFactorization.Core.PrintStats","text":"PrintStats(; kwargs...)\n\nDoes not use any of the kwargs. Simply prints the most recent row of the stats.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ProjectedNormalization","page":"Types","title":"BlockTensorFactorization.Core.ProjectedNormalization","text":"ProjectedNormalization(projection, norm; whats_normalized=identity)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be the same size as the output of whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ProjectedNormalization-Union{Tuple{ScaledNormalization{T}}, Tuple{T}} where T<:Real","page":"Types","title":"BlockTensorFactorization.Core.ProjectedNormalization","text":"ProjectedNormalization(S::ScaledNormalization{T}) where {T <: Real}\n\nConvert from a ScaledNormalization to a ProjectedNormalization.\n\nOnly works when the scale is 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Projection","page":"Types","title":"BlockTensorFactorization.Core.Projection","text":"Perform a projected gradient update on the nth factor of an Abstract Decomposition x\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.RelativeError","page":"Types","title":"BlockTensorFactorization.Core.RelativeError","text":"RelativeError{T<:Function} <: AbstractStat\n\nRelative error between the decomposition model, and input array.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Rescale","page":"Types","title":"BlockTensorFactorization.Core.Rescale","text":"Rescale{T<:Union{Nothing,Missing,Function}} <: ConstraintUpdate\nRescale(n, scale::ScaledNormalization, whats_rescaled::T)\n\nApplies the scaled normalization scale to factor n, and tries to multiply the scaling of factor n to other factors.\n\nIf whats_rescaled=nothing, then it will not rescale any other factor.\n\nIf whats_rescaled=missing, then it will try to evenly distribute the weight to all other factors using the (N-1) root of each weight where N is the number of factors. If the weights are not broadcastable, (e.g. you want to scale each row but each factor has a different number of rows), will use the geometric mean of the weights as the single weight to distribute evenly among the other factors.\n\nIf typeof(whats_rescaled) <: Function, will broadcast the weight to the output of calling this function on the entire decomposition. For example,     whats_rescale = x -> eachcol(factor(x, 2)) will rescale each column of the second factor of the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ScaledNormalization","page":"Types","title":"BlockTensorFactorization.Core.ScaledNormalization","text":"ScaledNormalization(norm; whats_normalized=identity, scale=1)\n\nMain constructor for the constraint where norm of whats_normalized equals scale.\n\nScale can be a single Real, or an AbstractArray{<:Real}, but should be broadcast-able with the output of whats_normalized. Lastly, scale can be a Function which will act on an AbstractArray{<:Real} and return something that is broadcast-able whats_normalized.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.ScaledNormalization-Tuple{ProjectedNormalization}","page":"Types","title":"BlockTensorFactorization.Core.ScaledNormalization","text":"ScaledNormalization(P::ProjectedNormalization)\n\nConvert from a ProjectedNormalization to a ScaledNormalization.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.SingletonDecomposition","page":"Types","title":"BlockTensorFactorization.Core.SingletonDecomposition","text":"SingletonDecomposition(A::AbstractArray, frozen=false)\n\nWraps an AbstractArray so it can be treated like an AbstractDecomposition\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.SuperDiagonal","page":"Types","title":"BlockTensorFactorization.Core.SuperDiagonal","text":"SuperDiagonal(v::AbstractVector, ndims::Integer=2)\n\nConstructs a SuperDiagonal array from the vector v.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.SuperDiagonal-2","page":"Types","title":"BlockTensorFactorization.Core.SuperDiagonal","text":"SuperDiagonal{T, N, V<:AbstractVector{T}} <: AbstractArray{T, N}\n\nArray of order N that is zero everywhere except possibly along the super diagonal.\n\n\n\n\n\n","category":"type"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker-Union{Tuple{N}, Tuple{NTuple{N, Integer}, NTuple{N, Integer}}} where N","page":"Types","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker(full_size::NTuple{N, Integer}, ranks::NTuple{N, Integer};\n    frozen=false_tuple(length(ranks)+1), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker type using init to initialize the factors.\n\nSee Tucker1.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker-Union{Tuple{Tuple{Vararg{AbstractArray{T}}}}, Tuple{T}, Tuple{Tuple{Vararg{AbstractArray{T}}}, Any}} where T","page":"Types","title":"BlockTensorFactorization.Core.Tucker","text":"Tucker((G, A, B, ...))\nTucker((G, A, B, ...), frozen)\n\nTucker decomposition. Takes the form of a core G times a matrix for each dimension.\n\nFor example, a rank (r, s, t) Tucker decomposition of an order three tensor D would be, entry-wise,\n\nD[i, j, k] = ∑_r ∑_s ∑_t G[r, s, t] * A[i, r] * B[j, s] * C[k, t].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee tuckerproduct.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker1-Union{Tuple{N}, Tuple{NTuple{N, Integer}, Integer}} where N","page":"Types","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1(full_size::NTuple{N, Integer}, rank::Integer; frozen=false_tuple(2), init=DEFAULT_INIT, kwargs...) where N\n\nConstructs a random Tucker1 type using init to initialize the factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/types/#BlockTensorFactorization.Core.Tucker1-Union{Tuple{Tuple{AbstractArray{T}, AbstractMatrix{T}}}, Tuple{T}, Tuple{Tuple{AbstractArray{T}, AbstractMatrix{T}}, Any}} where T","page":"Types","title":"BlockTensorFactorization.Core.Tucker1","text":"Tucker1((G, A))\nTucker1((G, A), frozen)\n\nTucker-1 decomposition. Takes the form of a core G times a matrix A. Entry-wise\n\nD[i₁, …, i_N] = ∑_r G[r, i₂, …, i_N] * A[i₁, r].\n\nOptionally use frozen::Tuple{Bool} to specify which factors are frozen.\n\nSee ×₁ and mtt.\n\n\n\n\n\n","category":"method"},{"location":"tutorial/iterationstats/#Iteration-Stats","page":"Iteration Stats","title":"Iteration Stats","text":"The following stats are supported inputs to the stats keyword in factorize.\n\nIteration\nGradientNorm\nGradientNNCone\nObjectiveValue\nObjectiveRatio\nRelativeError\nIterateNormDiff\nIterateRelativeDiff\nEuclideanStepSize\nEuclideanLipschitz\nFactorNorms\nPrintStats\nDisplayDecomposition","category":"section"},{"location":"tutorial/iterationstats/#Detail","page":"Iteration Stats","title":"Detail","text":"","category":"section"},{"location":"tutorial/iterationstats/#Auxiliary-Stats","page":"Iteration Stats","title":"Auxiliary Stats","text":"The following are subtype of AbstractStat but are for auxiliary features.","category":"section"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.AbstractStat-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.AbstractStat","text":"An AbstractStat is a type which, when created, can be applied to the four arguments (X::AbstractDecomposition, Y::AbstractArray, previous::Vector{<:AbstractDecomposition}, parameters::Dict) to (usually) return a number.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.Iteration-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.Iteration","text":"Iteration <: AbstractStat\n\nIteration number.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.GradientNorm-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.GradientNorm","text":"GradientNorm{T} <: AbstractStat\n\n2-norm of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.GradientNNCone-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.GradientNNCone","text":"GradientNNCone{T} <: AbstractStat\n\n2-norm vector-set distance between the negative gradient and nonnegative cone at the iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.ObjectiveValue-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.ObjectiveValue","text":"ObjectiveValue{T<:AbstractObjective} <: AbstractStat\n\nThe current objective value.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.ObjectiveRatio-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.ObjectiveRatio","text":"ObjectiveRatio{T<:AbstractObjective} <: AbstractStat\n\nRatio between the previous and current objective value.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.RelativeError-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.RelativeError","text":"RelativeError{T<:Function} <: AbstractStat\n\nRelative error between the decomposition model, and input array.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.IterateNormDiff-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.IterateNormDiff","text":"IterateNormDiff{T<:Function} <: AbstractStat\n\n2-norm of the difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.IterateRelativeDiff-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.IterateRelativeDiff","text":"IterateRelativeDiff{T<:Function} <: AbstractStat\n\nRelative difference between the previous and current iterate.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.EuclideanStepSize-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.EuclideanStepSize","text":"The 2-norm of the stepsizes that would be taken for all blocks.\n\nFor example, if there are two blocks, and we would take a stepsize of A to update one block and B to update the other, this would return sqrt(A^2 + B^2).\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.EuclideanLipschitz-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.EuclideanLipschitz","text":"The 2-norm of the lipschitz constants that would be taken for all blocks.\n\nNeed the stepsizes to be lipschitz steps since it is calculated similarly to EuclideanStepSize.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.FactorNorms-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.FactorNorms","text":"FactorNorms(; norm, kwargs...)\n\nMakes a tuple containing the norm of each factor in the decomposition.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.PrintStats-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.PrintStats","text":"PrintStats(; kwargs...)\n\nDoes not use any of the kwargs. Simply prints the most recent row of the stats.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/iterationstats/#BlockTensorFactorization.Core.DisplayDecomposition-tutorial-iterationstats","page":"Iteration Stats","title":"BlockTensorFactorization.Core.DisplayDecomposition","text":"DisplayDecomposition(; kwargs...)\n\nDoes not use any of the kwargs. Simply displays the current iteration.\n\n\n\n\n\n","category":"type"},{"location":"#Block-Tensor-Decomposition","page":"Home","title":"Block Tensor Decomposition","text":"","category":"section"},{"location":"#About-this-package","page":"Home","title":"About this package","text":"BlockTensorFactorization.jl is a package to factorize tensors. The main feature is its flexibility at decomposing input tensors according to many common tensor models (ex. CP, Tucker) with a number of constraints (ex. nonnegative, simplex).\n\n(Coming Soon) The package also supports user defined models and constraints provided the operations for combining factor into a tensor, and projecting/applying the constraint are given. It is also a longer term goal to support other optimization objective beyond minimizing the least-squares (Frobenius norm) between the input tensor and model.\n\nThe general scheme for computing the decomposition is a generalization of Xu and Yin's Block Coordinate Descent Method (2013) that cyclicaly updates each factor in a model with a proximal gradient descent step. Note for convex constraints, the proximal operation would be a Euclidean projection onto the constraint set, but we find some improvment with a hybrid approach of a partial Euclidean projection followed by a rescaling step. In the case of a simplex constraint on one factor, this looks like: dividing the constrained factor by the sum of entries, and multiplying another factor by this sum to preserve the product.\n\nDepth = 3","category":"section"},{"location":"reference/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₁-Tuple{AbstractArray, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.:×₁","text":"1-mode product between a tensor and a matrix\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₂","page":"Functions","title":"BlockTensorFactorization.Core.:×₂","text":"2-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₃","page":"Functions","title":"BlockTensorFactorization.Core.:×₃","text":"3-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₄","page":"Functions","title":"BlockTensorFactorization.Core.:×₄","text":"4-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₅","page":"Functions","title":"BlockTensorFactorization.Core.:×₅","text":"5-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₆","page":"Functions","title":"BlockTensorFactorization.Core.:×₆","text":"6-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₇","page":"Functions","title":"BlockTensorFactorization.Core.:×₇","text":"7-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₈","page":"Functions","title":"BlockTensorFactorization.Core.:×₈","text":"8-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:×₉","page":"Functions","title":"BlockTensorFactorization.Core.:×₉","text":"9-mode product between a tensor and a matrix. See nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₁","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₁","text":"Slice-wise dot along mode 1. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₂","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₂","text":"Slice-wise dot along mode 2. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₃","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₃","text":"Slice-wise dot along mode 3. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₄","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₄","text":"Slice-wise dot along mode 4. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₅","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₅","text":"Slice-wise dot along mode 5. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₆","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₆","text":"Slice-wise dot along mode 6. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₇","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₇","text":"Slice-wise dot along mode 7. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₈","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₈","text":"Slice-wise dot along mode 8. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.:⋅₉","page":"Functions","title":"BlockTensorFactorization.Core.:⋅₉","text":"Slice-wise dot along mode 9. See slicewise_dot.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.DEFAULT_INIT","page":"Functions","title":"BlockTensorFactorization.Core.DEFAULT_INIT","text":"Default initialization function to use when creating a random decomposition.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.Diagonal_col_norm-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.Diagonal_col_norm","text":"Diagonal_col_norm(X)\n\nCalculates a diagonal matrix with entries that are the Euclidean norm of each column of X.\n\nShorthand for Diagonal(norm.(eachcol(X))).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.IntervalConstraint-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.IntervalConstraint","text":"Entrywise(x -> clamp(x, a, b), x -> a ≤ x ≤ b)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.ReLU-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.ReLU","text":"max(0,x)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core._factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core._factorize","text":"Inner level function once keyword arguments are set\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core._gettuckerindex-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core._gettuckerindex","text":"Just computes index I in the tucker product\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.abs_randn-Tuple","page":"Functions","title":"BlockTensorFactorization.Core.abs_randn","text":"abs_randn(x...)\n\nFolded normal or more specificly the half-normal initialization.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.all_recursive-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.all_recursive","text":"all_recursive(x)\nall_recursive(f, x)\n\nLike all but checks recursively on nested types like arrays of vectors, tuples of sets of arrays, etc.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.array-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.array","text":"array(D::AbstractDecomposition)\n\nTurns a decomposition into the full array, usually by multiplying the factors to reconstruct the full array.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.binary!","page":"Functions","title":"BlockTensorFactorization.Core.binary!","text":"Entrywise(binaryproject, x -> x in (0, 1))\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.check-Tuple{AbstractConstraint, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.check","text":"check(C::AbstractConstraint, A::AbstractArray)::Bool\n\nReturns true if A satisfies the constraint C.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.check-Tuple{Entrywise, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.check","text":"check(C::Entrywise, A::AbstractArray)::Bool\n\nChecks if A is entry-wise constrained.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.circumscribed_radius-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.circumscribed_radius","text":"Finds the radius of the circumscribed circle between points (a,f), (b,g), (c,h)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.coarsen-Tuple{AbstractArray, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.coarsen","text":"coarsen(Y::AbstractArray, scale::Integer; dims=1:ndims(Y))\n\nCoarsens or downsamples Y by scale. Only keeps every scale entries along the dimensions specified.\n\nExample\n\nY = randn(12, 12, 12)\n\ncoarsen(Y, 2) == Y[begin:2:end, begin:2:end, begin:2:end]\n\ncoarsen(Y, 4; dims=(1, 3)) == Y[begin:4:end, :, begin:4:end]\n\ncoarsen(Y, 3; dims=2) == Y[:, begin:3:end, :]\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.contractions-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.contractions","text":"contractions(D::AbstractDecomposition)\n\nA tuple of functions defining a recipe for reconstructing a full array from the factors of the decomposition.\n\nExample\n\n(op1, op2) = contractions(D) (A, B, C) = factors(D)\n\narray(D) == (A op1 B) op2 C\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.core-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.core","text":"core(T::AbstractTucker)\n\nThe core of a Tucker-like decomposition. Same number of dimensions as the full array.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.cpproduct-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.cpproduct","text":"cpproduct((A, B, C, ...))\ncpproduct(A, B, C, ...)\n\nMultiplies the inputs by treating them as matrices in a CP decomposition.\n\nExample\n\ncpproduct(A, B, C) == @einsum T[i, j, k] := A[i, r] * B[j, r] * C[k, r]\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.cubic_spline_coefficients-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.cubic_spline_coefficients","text":"cubic_spline_coefficients(y::AbstractVector{<:Real}; h=1)\n\nCalculates the list of coefficients a, b, c, d for an interpolating spline.\n\nThe spline is defined as f(x) = g_i(x) on xi leq x leq xi+1 where\n\ng_i(x) = ai(x-xi)^3 + bi(x-xi)^2 + ci(x-xi) + di\n\nUses the following boundary conditions\n\ng_1(x1-h) = 1 (i.e. the y-intercept is (01) for uniform spaced x=1:n)\ng_n(xn+h) = xn (i.e. repeated right end-point)\ng_n(xn+h) = 0 (i.e. flat/no-curvature one spacing after end-point)\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.curvature-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.curvature","text":"curvature(y::AbstractVector{<:Real})\n\nApproximates the signed curvature of a function given evenly spaced samples.\n\nUses d_dx and d2_dx2 to approximate the first two derivatives.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d2_dx2-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d2_dx2","text":"d2_dx2(y::AbstractVector{<:Real})\n\nApproximate second derivative with finite elements. Assumes y[i] = y(xi) are samples with unit spaced inputs x{i+1} - x_i = 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d_dx-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d_dx","text":"d_dx(y::AbstractVector{<:Real})\n\nApproximate first derivative with finite elements. Assumes y[i] = y(xi) are samples with unit spaced inputs x{i+1} - x_i = 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.d_dx_and_d2_dx2_spline-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.d_dx_and_d2_dx2_spline","text":"Extracts the first and second derivatives of the splines at the knots\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.default_kwargs-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.default_kwargs","text":"default_kwargs(Y; kwargs...)\n\nHandles all keywords and options, and sets defaults if not provided.\n\nKeywords & Defaults\n\nInitialization\n\ndecomposition: nothing. Can provide a custom initialized AbstractDecomposition. Note this exact decomposition is mutated.\nmodel: Tucker1, but overridden by the type of AbstractDecomposition if given decomposition\nrank: nothing, but overridden by the rank of AbstractDecomposition if given decomposition. Automatically calls rank_detect_factorize if both rank & decomposition are not provided.\ninit: abs_randn for nonnegative inputs Y, randn otherwise\nconstrain_init: true. Ensures the initialization satisfies all given constraints. Defaults to false if given decomposition\nfreeze: the default frozen factors of the model\ncontinuous_dims: missing. Dimensions of Y that come from discretizations of continuous data. If provided, multiscale_factorize is called and can speed up factorization. If continuous_dims==nothing, factorization will only happen at one scale. In the future, if continuous_dims==missing,factorize may guess if there are continuous dimensions.\n\nUpdates\n\nobjective: L2(). Objective to minimize\nnorm: l2norm. Norm to use for statistics, can be unrelated to the objective\nrandom_order: false. Perform the updates in a random order each iteration, Overrides to true when recursive_random_order=true\ngroup_updates_by_factor: false. Groups updates on the same factor together. Overrides to true when random_order=true. Useful when randomizing order of updates but you want to keep matching momentum-gradientstep-constraint together\nrecursive_random_order: false. Performs inner blocked updates (grouped updates) in a random order (recursively) each iteration. Note the outer most list of updates can be performed in order if random_order=false\ndo_subblock_updates: false. Performs gradient descent on subblocks within a factor separately. May result in smaller Lipschitz constants and hence larger step sizes being used.\n\nMomentum\n\nmomentum: true\nδ: 0.9999. Amount of momentum, between [0,1)\nprevious_iterates: 1. Number of pervious iterates to save and use between iterations\n\nConstraints\n\nconstraints: nothing. Can be a list of ConstraintUpdate, or just one\nfinal_constraints: nothing. Constraints to apply after the final iteration. Will apply constraints if constrain_output is true and none are given\nconstrain_output: false. Apply the final_constraints, will override to true if final_constraints are given\n\nStats\n\nstats: [Iteration, ObjectiveValue, GradientNorm] or in the case of nonnegative Y, GradientNNCone in place of GradientNorm\nconverged: GradientNorm or in the case of nonnegative Y, GradientNNCone. What stat(s) to use for convergence. Will converge is any one of the provided stats is below their respective tolerance\ntolerance: 1. A list the same length as converged\nmaxiter: 1000. Additional stopping criterion if the number of iterations exceeds this number\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachfactorindex-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.eachfactorindex","text":"eachfactorindex(D::AbstractDecomposition)\n\nAn iterable to index the factors of D; not necessarily 1:ndims(D). Does not include non-data factors like the core of a CPDecomposition.\n\nFor example, eachfactorindex(D::Tucker) == 0:ndims(D) since core(D) is the zeroth factor. eachfactorindex(D::CPDecomposition) == 1:ndims(D). core(D) exists, but it is frozen as the identity tensor. And eachfactorindex(D::Tucker1) == 0:1 representing the core and matrix factor of D.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachfibre-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.eachfibre","text":"eachfibre(A::AbstractArray; n::Integer, kwargs...)\n\nCreates views of A that are that n-fibres of A.\n\nShorthand for eachslice(A; dims=(1,...,n-1,n+1,...,ndims(A))).\n\nSee eachslice.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::AbstractTucker)\n\nCreates a generator for each rank 1 term of a Tucker decomposition.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::CPDecomposition)\n\nThe (CP) rank-1 tensors Tr[i1, ..., iN] = A1[i1, r] * … * AN[iN, r]  for each r = 1, …, rankof(T).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.eachrank1term-Tuple{Tucker1}","page":"Functions","title":"BlockTensorFactorization.Core.eachrank1term","text":"eachrank1term(T::Tucker1)\n\nThe (Tucker-1) rank-1 tensors Tr[i1, ..., iN] = A[i1, r] * B[r, i2, ..., iN] for each r = 1, …, rankof(T).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.expand_decomposition_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.expand_decomposition_constraints","text":"Use the same initialization as factorize() to get the expanded set of constraints\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factor-Tuple{AbstractDecomposition, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.factor","text":"factor(D::AbstractDecomposition, n::Integer)\n\nThe nth factor of D.\n\nUse factors to get all the factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.factorize","text":"factorize(Y; rank=nothing, model=Tucker1, kwargs...)\n\nFactorizes Y according to the decomposition model.\n\nSee default_kwargs for the default keywords.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.factors-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.factors","text":"factors(D::AbstractDecomposition)\n\nA tuple of arrays representing the decomposition of D.\n\nUse factor to get just the nth factor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.false_tuple-Tuple{Integer}","page":"Functions","title":"BlockTensorFactorization.Core.false_tuple","text":"Makes a Tuple of length n filled with false.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.finalconstrain!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.finalconstrain!","text":"finalconstrain!(decomposition; constraints, final_constraints, kwargs...)\n\nApplies final_constraints (or if its nothing, applies constraints) to the decomposition.\n\nAny RescaleUpdate are applied (the factor is scaled), but the rescaling of other factors is skipped.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.frozen-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.frozen","text":"frozen(D::AbstractDecomposition)\n\nA tuple of Bools the same length as factors(D) showing which factors are \"frozen\" in the sense that a block decent algorithm should skip these factors when decomposing a tensor.\n\nSee isfrozen.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.geomean-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.geomean","text":"geomean(v)\ngeomean(v...)\n\nGeometric mean of a collection: prod(v)^(1/length(v)).\n\nIf prod(v) is detected to be 0 or Inf, the safer (but slower) implementation exp(mean(log.(v))) is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.getnotindex-Tuple{Any, Int64}","page":"Functions","title":"BlockTensorFactorization.Core.getnotindex","text":"Like getindex but returns the compliment to the index or indices requested.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.group_by_factor-Tuple{BlockedUpdate}","page":"Functions","title":"BlockTensorFactorization.Core.group_by_factor","text":"group_by_factor(blockedupdate::BlockedUpdate)\n\nGroups updates according to the factor they operate on.\n\nIf blockedupdate contains other BlockedUpdates, the inner updates are grouped when they all operate on the same factor.\n\nUpdates which do not have an assigned factor are grouped together.\n\nThe order which these groups appear in the output follows the same order as the first appearence of each unique factor that is operated on.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.identity_tensor-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.identity_tensor","text":"identity_tensor(I, ndims)\nidentity_tensor(T, I, ndims)\n\nCreates a SuperDiagonal array of ones with size I × ... × I of order ndims.\n\nCan provide a type T for the identity tensor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.identityslice-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Functions","title":"BlockTensorFactorization.Core.identityslice","text":"identityslice(x::AbstractArray{T, N})\n\nUseful for returning an iterable with a single iterate x\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize","text":"Main initialization function for factorize.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_continuous_dims-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_continuous_dims","text":"initialize_continuous_dims(Y; kwargs...)\n\nLists dimensions of Y that represent a discretization of a continuous function.\n\nDefaults to all of them: continuous_dims = 1:ndims(Y).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_decomposition-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_decomposition","text":"The decomposition model Y will be factored into\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_parameters-Tuple{Any, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_parameters","text":"update parameters needed for the update\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_previous-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_previous","text":"Keep track of one or more previous iterates\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_scales-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_scales","text":"initialize_scales(Y, kwargs)\n\nInitializes the plan for factorizing at progressively finer scales.\n\nThe list of scales should be ordered from largest (coarse) to smallest (fine) and end with 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.initialize_stats-NTuple{4, Any}","page":"Functions","title":"BlockTensorFactorization.Core.initialize_stats","text":"The stats that will be saved every iteration\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.interlace-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.interlace","text":"interlace(u, v)\n\nTakes two iterables, u and v, and alternates elements from u and v into a vector. If u and v are not the same length, extra elements are put on the end of the vector.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.interpolate-Tuple{AbstractArray, Any}","page":"Functions","title":"BlockTensorFactorization.Core.interpolate","text":"interpolate(Y, scale; dims=1:ndims(Y), degree=0, kwargs...)\n\nInterpolates Y to a larger array with repeated values.\n\nKeywords\n\nscale. How much to scale up the size of Y. A dimension with size k will be scaled to scale*k - (scale - 1) = scale*(k-1) + 1\n\ndims:1:ndims(Y). Which dimensions to interpolate.\n\ndegree:0. What degree of interpolation to use. 0 is constant interpolation, 1 is linear.\n\nLike the opposite of coarsen.\n\nExample\n\njulia> Y = collect(reshape(1:6, 2, 3))\n2×3 Matrix{Int64}:\n 1  3  5\n 2  4  6\n\njulia> interpolate(Y, 2)\n3×5 Matrix{Int64}:\n 1  1  3  3  5\n 1  1  3  3  5\n 2  2  4  4  6\n\njulia> interpolate(Y, 3; dims=2)\n2×7 Matrix{Int64}:\n 1  1  1  3  3  3  5\n 2  2  2  4  4  4  6\n\njulia> interpolate(Y, 1) == Y\ntrue\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isfrozen-Tuple{AbstractDecomposition, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.isfrozen","text":"isfrozen(D::AbstractDecomposition, n::Integer)\n\nTrue if the nth factor of D is frozen. See frozen.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isnonnegative-Tuple{AbstractArray{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.isnonnegative","text":"isnonnegative(X::AbstractArray{<:Real})\nisnonnegative(x::Real)\n\nChecks if all entries of X are bigger or equal to zero. Will be a standard function in Base but using this for now: https://github.com/JuliaLang/julia/pull/53677\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.isnonnegative_sumtoone-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.isnonnegative_sumtoone","text":"all(isnonnegative, x) && sum(x) ≈ 1\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.khatrirao-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.khatrirao","text":"khatrirao(A::AbstractMatrix, B::AbstractMatrix)\nA ⊙ B\n\nKhatri-Rao product of two matrices. A ⊙ B can be typed with \\odot.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize!","text":"l1normalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_12slices!","text":"l1normalize_12slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=each12slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_1slices!","text":"l1normalize_1slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=each1slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_cols!","text":"l1normalize_cols! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=eachcol).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1normalize_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l1normalize_rows!","text":"l1normalize_rows! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l1norm, l1project!; whats_normalized=eachrow).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale!","text":"l1scale! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=identityslice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_12slices!","text":"l1scale_12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=each12slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_1slices!","text":"l1scale_1slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=each1slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_average12slices!","text":"l1scale_average12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=each1slice, scale=size2).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_cols!","text":"l1scale_cols! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=eachcol).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l1scale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l1scale_rows!","text":"l1scale_rows! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l1norm; whats_normalized=eachrow).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2normalize!","page":"Functions","title":"BlockTensorFactorization.Core.l2normalize!","text":"l2normalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2normalize_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2normalize_12slices!","text":"l2normalize_12slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=each12slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2normalize_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2normalize_1slices!","text":"l2normalize_1slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=each1slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2normalize_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l2normalize_cols!","text":"l2normalize_cols! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=eachcol).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2normalize_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l2normalize_rows!","text":"l2normalize_rows! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(l2norm, l2project!; whats_normalized=eachrow).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale!","text":"l2scale! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=identityslice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_12slices!","text":"l2scale_12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=each12slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_1slices!","text":"l2scale_1slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=each1slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_average12slices!","text":"l2scale_average12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=each1slice, scale=size2).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_cols!","text":"l2scale_cols! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=eachcol).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.l2scale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.l2scale_rows!","text":"l2scale_rows! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(l2norm; whats_normalized=eachrow).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynorm-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.linftynorm","text":"linftynorm(x::AbstractArray)\n\nCalculates ‖x‖_∞.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize!","text":"linftynormalize! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_12slices!","text":"linftynormalize_12slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=each12slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_1slices!","text":"linftynormalize_1slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=each1slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_cols!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_cols!","text":"linftynormalize_cols! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=eachcol).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftynormalize_rows!","page":"Functions","title":"BlockTensorFactorization.Core.linftynormalize_rows!","text":"linftynormalize_rows! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(linftynorm, linftyproject!; whats_normalized=eachrow).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyproject!-Tuple{AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.linftyproject!","text":"linftyproject!(x::AbstractArray)\n\nEuclidean projection onto the unit L∞-ball.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale!","text":"linftyscale! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=identityslice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_12slices!","text":"linftyscale_12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=each12slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_1slices!","text":"linftyscale_1slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=each1slice).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_average12slices!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_average12slices!","text":"linftyscale_average12slices! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=each1slice, scale=size2).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_cols!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_cols!","text":"linftyscale_cols! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=eachcol).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.linftyscale_rows!","page":"Functions","title":"BlockTensorFactorization.Core.linftyscale_rows!","text":"linftyscale_rows! <: ScaledNormalization\n\nAlias for\n\nScaledNormalization(linftynorm; whats_normalized=eachrow).\n\nSee ScaledNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.make_spline-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.make_spline","text":"make_spline(y::AbstractVector{<:Real}; h=1)\n\nReturns a function f(x) that is an interpolating/extrapolating spline for y, with uniform stepsize h between the x-values of the knots.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.make_update!-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.make_update!","text":"What one iteration of the algorithm looks like. One iteration is likely a full cycle through each block or factor of the model.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.mat-Tuple{AbstractArray, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.mat","text":"mat(A::AbstractArray, n::Integer)\n\nMatricize along the nth mode.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_cols!-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.match_cols!","text":"match_cols!(X::AbstractMatrix, Y::AbstractMatrix; kwargs...)\n\nmatch_slices! along the second dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_factors!-Union{Tuple{T}, Tuple{T, T}} where T<:AbstractDecomposition","page":"Functions","title":"BlockTensorFactorization.Core.match_factors!","text":"match_factors!(X::T, Y::T; dist=L2) where {T <: AbstractDecomposition}\n\nReorders the rank-1 terms of X to best match the slices of Y.\n\nThis is currently implemented for Tucker1 and CPDecomposition.\n\nSee match_slices!.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_rows!-Tuple{AbstractMatrix, AbstractMatrix}","page":"Functions","title":"BlockTensorFactorization.Core.match_rows!","text":"match_rows!(X::AbstractMatrix, Y::AbstractMatrix; kwargs...)\n\nmatch_slices! along the first dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.match_slices!-Tuple{AbstractArray, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.match_slices!","text":"match_slices!(X, Y; dims, dist=L2)\n\nReorders the order-dims slices of X to best match the slices of Y.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.matrix_factor-Tuple{AbstractTucker, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.matrix_factor","text":"matrix_factor(T::AbstractTucker, n::Integer)\n\nThe nth matrix factor. See matrix_factors.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.matrix_factors-Tuple{AbstractTucker}","page":"Functions","title":"BlockTensorFactorization.Core.matrix_factors","text":"matrix_factors(T::AbstractTucker)\n\nTuple of the non-core factors of T. See matrix_factor.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.mtt-Tuple{AbstractMatrix, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.mtt","text":"mtt(A::AbstractMatrix, B::AbstractArray)\n\nMatrix Times Tensor. Entry-wise,\n\nmtt(A, B)[i1, i2, …, iN] = ∑_r A[i1, r] * B[r, i2, …, iN].\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.multifoldl-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.multifoldl","text":"multifoldl(ops, args)\n\nLike foldl, but with a different folding operation between each argument.\n\nExample\n\njulia> multifoldl((+,*,-), (2,3,4,5))\n15\n\njulia> ((2 + 3) * 4) - 5\n15\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.multiscale_factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.multiscale_factorize","text":"multiscale_factorize(Y; continuous_dims=1:ndims(Y), rank=1, model=Tucker1, kwargs...)\n\nLike factorize but uses progressively finer sub-grids of Y to speed up convergence. This is only effective when the dimensions given by dims come from discretizations of continuous data.\n\nFor example, if Y has 3 dimensions where Y[i, j, k] are samples from a continuous 2D function fi(xj, y_k) on a grid, use\n\nmultiscale_factorize(Y; continuous_dims=(2,3))\n\nsince second and third dimensions are continuous.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nfactors-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.nfactors","text":"nfactors(D::AbstractDecomposition)\n\nReturns the number of factors/blocks in a decomposition.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmode_product-Tuple{AbstractArray, AbstractMatrix, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.nmode_product","text":"nmode_product(A::AbstractArray, B::AbstractMatrix, n::Integer)\n\nContracts the nth mode of A with the first mode of B. Equivalent to A ×ₙ B where\n\n(A ×ₙ B)[i₁, …, i_N] = ∑ⱼ A[i₁, …, iₙ₋₁, j, iₙ₊₁, …, i_N] B[iₙ, j].\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmode_product-Tuple{AbstractArray, AbstractVector, Integer}","page":"Functions","title":"BlockTensorFactorization.Core.nmode_product","text":"nmode_product(A::AbstractArray, b::AbstractVector, n::Integer)\n\nContracts the nth mode of A with b. Equivalent to A ×ₙ b where\n\n(A ×ₙ b)[i₁, …, iₙ₋₁, iₙ₊₁, …, i_N] = ∑_iₙ A[i₁, …, i_N] b[iₙ].\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.nmp","page":"Functions","title":"BlockTensorFactorization.Core.nmp","text":"Shorthand for nmode_product.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.nonnegative!","page":"Functions","title":"BlockTensorFactorization.Core.nonnegative!","text":"Entrywise(ReLU, isnonnegative)\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.norm2-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.norm2","text":"norm2(x)\n\nL2 norm squared, the sum of squares of the entries of x.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.outer_product-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.outer_product","text":"outer_product(vectors)\nouter_product(vectors...)\n\nOuter product of a collection of vectors.\n\nFor example,\n\nouter_product(u, v) == u * v'\n\nand\n\nouter_product(u, v, w)[i, j, k] == u[i] * v[j] * w[k].\n\nReturned array will have same number dimensions as the length of the collection.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.parse_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.parse_constraints","text":"parse_constraints(constraints, decomposition; kwargs...)\n\nParses the constraints to make sure we have a valid list of ConstraintUpdate.\n\nIf only one AbstractConstraint is given, assume we want this constraint to apply to every factor in the decomposition, and make a ConstraintUpdate for each factor.\n\nIf we are given a list of AbstractConstraint, assume we want them to apply to each factor of the decomposition in order.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.possible_ranks-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.possible_ranks","text":"possible_ranks(Y, model)\n\nReturns the rank of possible ranks Y could have under the model.\n\nFor matrices I × J this is 1:min(I, J). This is can be extended to tensors for different type of decompositions.\n\nTucker-1 rank is ≤ min(I, prod(J1,...,JN)) for tensors I × J1 × … × JN.\n\nThe CP-rank is ≤ minimum_{n} (prod(I1,...,IN) / In) for tensors I1 × … × IN in general. Although some shapes have have tighter upper bounds. For example, 2 × I × I tensors over ℝ have a maximum rank of floor(3I/2).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.postprocess!-NTuple{8, Any}","page":"Functions","title":"BlockTensorFactorization.Core.postprocess!","text":"Any post algorithm processing that needs to be done in factorize.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.proj_one_hot!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.proj_one_hot!","text":"proj_one_hot!(x)\n\nMutating version of proj_one_hot.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.proj_one_hot-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.proj_one_hot","text":"proj_one_hot(x)\n\nProjects an array x to the closest one hot array: an array with all 0's except for a single 1. Does not mutate; see proj_one_hot! for a mutating version.\n\nThis is not a unique projection if there are multiple largest entries. In this case, will pick one of the largest entries to be 1 and set the rest to zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.projsplx!-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.projsplx!","text":"projsplx!(y; sum=one(eltype(y)))\n\nProjects (in Euclidean distance) the array y into the simplex.\n\nSee projsplx for a non-mutating version.\n\n[1] Yunmei Chen and Xiaojing Ye, \"Projection Onto A Simplex\", 2011\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.projsplx-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.projsplx","text":"projsplx(y; sum=one(eltype(y)))\n\nNon-mutating version of projsplx!.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rank_detect_factorize-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.rank_detect_factorize","text":"rank_detect_factorize(Y; online_rank_estimation=false, rank=nothing, model=Tucker1, kwargs...)\n\nWraps factorize() with rank detection.\n\nSelects the rank that maximizes the standard curvature of the Relative Error (as a function of rank).\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rankof-Tuple{AbstractDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.rankof","text":"rankof(D::AbstractDecomposition)\n\nInternal dimension sizes for a decomposition. Returns the sizes of all factors if not defined for a concrete subtype of AbstractDecomposition.\n\nExamples\n\nCPDecomposition: size of second dimension for the factors Tucker: size of the core factor Tucker1: size of the first dimension of the core factor\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.rankof-Tuple{CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.rankof","text":"The single rank for a CP Decomposition\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.reshape_ndims-Tuple{AbstractArray, Any}","page":"Functions","title":"BlockTensorFactorization.Core.reshape_ndims","text":"reshape_ndims(x, n)\n\nReshapes x to a higher order array with n dimensions.\n\nWhen n > ndims(x), extra dimensions are prepended. Otherwise, trailing dimensions are collapsed.\n\nExample\n\njulia> x = [1, 2, 3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> reshape_ndims(x, 1)\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> reshape_ndims(x, 2)\n1×3 Matrix{Int64}:\n 1  2  3\n\njulia> reshape_ndims(x, 3)\n1×1×3 Array{Int64, 3}:\n[:, :, 1] =\n 1\n\n[:, :, 2] =\n 2\n\n[:, :, 3] =\n 3\n\njulia> A = reshape(collect(1:12), 2, 2, 3)\n2×2×3 Array{Int64, 3}:\n[:, :, 1] =\n 1  3\n 2  4\n\n[:, :, 2] =\n 5  7\n 6  8\n\n[:, :, 3] =\n  9  11\n 10  12\n\njulia> reshape_ndims(A, 2)\n2×6 Matrix{Int64}:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\nCredit: https://discourse.julialang.org/t/outer-product-broadcast/103731/7\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_constraint-Tuple{AbstractConstraint, Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.scale_constraint","text":"scale_constraint(constraint::AbstractConstraint, scale, n_continuous_dims)\n\nReturns a scaled version of the constraint based off the number of relevant continuous dimensions the constraint acts on.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_constraints-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.scale_constraints","text":"scale_constraints(Y, scale; kwargs...)\n\nScales any constraints that need to be modified to use at a coarser scale.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.scale_decomposition_constraint-Tuple{Any, Any, Any, CPDecomposition}","page":"Functions","title":"BlockTensorFactorization.Core.scale_decomposition_constraint","text":"Idea is that external dimensions (I₁, I₂, ...) that are continuous dimensions need to be scaled, but internal dimensions (R₁, R₂, ...) or non-continuous dimensions don't.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex!","page":"Functions","title":"BlockTensorFactorization.Core.simplex!","text":"simplex! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(isnonnegative_sumtoone, projsplx!; whats_normalized=identityslice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_12slices!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_12slices!","text":"simplex_12slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(isnonnegative_sumtoone, projsplx!; whats_normalized=each12slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_1slices!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_1slices!","text":"simplex_1slices! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(isnonnegative_sumtoone, projsplx!; whats_normalized=each1slice).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_cols!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_cols!","text":"simplex_cols! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(isnonnegative_sumtoone, projsplx!; whats_normalized=eachcol).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.simplex_rows!","page":"Functions","title":"BlockTensorFactorization.Core.simplex_rows!","text":"simplex_rows! <: ProjectedNormalization\n\nAlias for\n\nProjectedNormalization(isnonnegative_sumtoone, projsplx!; whats_normalized=eachrow).\n\nSee ProjectedNormalization.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.slicewise_dot-Tuple{AbstractArray, AbstractArray}","page":"Functions","title":"BlockTensorFactorization.Core.slicewise_dot","text":"slicewise_dot(A::AbstractArray, B::AbstractArray; dims=1, dimsA=dims, dimsB=dims)\n\nContracts all but the dimensions dimsA and dimsB of A and B. Entry-wise\n\n(slicewise_dot(A,B; dimsA, dimsB))[i_dimsA, i_dimsB] = A[…, i_dimsA, ⋯] ⋅ B[…, i_dimsB, ⋯].\n\nWhen dims==n, equivalent to A ⋅ₙ B. If A and B are both matrices, slicewise_dot(A, B) == A'B.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.smart_insert!-Tuple{BlockedUpdate, AbstractUpdate}","page":"Functions","title":"BlockTensorFactorization.Core.smart_insert!","text":"smart_insert!(U::BlockedUpdate, V::AbstractUpdate)\n\nTries to insert V into U after the last matching update in U. A \"matching update\" means it updates the same factor/block n. See smart_interlace!\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.smart_interlace!-Tuple{BlockedUpdate, Any}","page":"Functions","title":"BlockTensorFactorization.Core.smart_interlace!","text":"smart_interlace!(U::BlockedUpdate, V)\n\nsmart_insert!s each update in V, into U. See smart_insert!\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.spline_mat-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.spline_mat","text":"spline_mat(n)\n\nCreates the Tridiagonal matrix to solve for coefficients b. See cubic_spline_coefficients.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.standard_curvature-Tuple{AbstractVector{<:Real}}","page":"Functions","title":"BlockTensorFactorization.Core.standard_curvature","text":"standard_curvature(y::AbstractVector{<:Real})\n\nApproximates the signed curvature of a function, scaled to the unit box 01^2.\n\nSee curvature.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.swapdims","page":"Functions","title":"BlockTensorFactorization.Core.swapdims","text":"swapdims(A::AbstractArray, a::Integer, b::Integer=1)\n\nSwap dimensions a and b.\n\n\n\n\n\n","category":"function"},{"location":"reference/functions/#BlockTensorFactorization.Core.tucker_contractions-Tuple{Any}","page":"Functions","title":"BlockTensorFactorization.Core.tucker_contractions","text":"tucker_contractions(N)\n\nContractions used in a full Tucker decomposition of order N. This is a tuple of the n-mode products from 1 to N in order.\n\n\n\n\n\n","category":"method"},{"location":"reference/functions/#BlockTensorFactorization.Core.tuckerproduct-Tuple{Any, Any}","page":"Functions","title":"BlockTensorFactorization.Core.tuckerproduct","text":"tuckerproduct(G, (A, B, ...))\ntuckerproduct(G, A, B, ...)\n\nMultiplies the inputs by treating the first argument as the core and the rest of the arguments as matrices in a Tucker decomposition.\n\nExample\n\ntuckerproduct(G, (A, B, C)) == G ×₁ A ×₂ B ×₃ C tuckerproduct(G, (A, B, C); exclude=2) == G ×₁ A ×₃ C tuckerproduct(G, (A, B, C); exclude=2, excludes_missing=false) == G ×₁ A ×₃ C tuckerproduct(G, (A, C); exclude=2, excludes_missing=true) == G ×₁ A ×₃ C`\n\n\n\n\n\n","category":"method"},{"location":"tutorial/blockupdateorder/#Block-Update-Order","page":"Block Update Order","title":"Block Update Order","text":"The default order the blocks are updated is cyclically through each factor of the decomposition D::AbstractDecomposition, in the order of factors(D). For AbstractTucker decompositions like Tucker, Tucker-1, and CP, this means starting with the core, followed by the matrix factor for the first dimension, second dimension, and so on.\n\nAs an example, this would be the default order of updates for nonnegative CP decomposition on an order 3 tensor.\n\nBlockedUpdate(\n    MomentumUpdate(1, lipschitz)\n    GradientStep(1, gradient, LipschitzStep)\n    Projection(1, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(2, lipschitz)\n    GradientStep(2, gradient, LipschitzStep)\n    Projection(2, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(3, lipschitz)\n    GradientStep(3, gradient, LipschitzStep)\n    Projection(3, Entrywise(ReLU, isnonnegative))\n)","category":"section"},{"location":"tutorial/blockupdateorder/#Randomizing-Block-Updates-Order","page":"Block Update Order","title":"Randomizing Block Updates Order","text":"The order of updates can be randomized with the random_order keyword.\n\nX, stats, kwargs = factorize(Y; random_order=true)\n\nBy default, this will keep momentum steps, gradient steps, and constraint steps for each factor together as a block, in this order.\n\nA possible order of updates could be the following. Note that the updates for each factor are grouped together, but each factor is updated in a random order.\n\nBlockedUpdate(\n    BlockedUpdate(\n        MomentumUpdate(2, lipschitz)\n        GradientStep(2, gradient, LipschitzStep)\n        Projection(2, Entrywise(ReLU, isnonnegative))\n    )\n    BlockedUpdate(\n        MomentumUpdate(1, lipschitz)\n        GradientStep(1, gradient, LipschitzStep)\n        Projection(1, Entrywise(ReLU, isnonnegative))\n    )\n    BlockedUpdate(\n        MomentumUpdate(3, lipschitz)\n        GradientStep(3, gradient, LipschitzStep)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n    )\n)\n\nFor more randomization, use the recursive_random_order keyword which will also randomize the order in which the momentum steps, gradient steps, and constraint steps are performed.\n\nX, stats, kwargs = factorize(Y; recursive_random_order=true)\n\nA possible order of updates could now be the following. The updates for each factor are still grouped together, but the updates within each block appear in a random order.\n\nBlockedUpdate(\n    BlockedUpdate(\n        Projection(2, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(2, lipschitz)\n        GradientStep(2, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        MomentumUpdate(1, lipschitz)\n        Projection(1, Entrywise(ReLU, isnonnegative))\n        GradientStep(1, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        GradientStep(3, gradient, LipschitzStep)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(3, lipschitz)\n    )\n)\n\nThe opposite of this would be to keep the outer order of blocks as given, but randomize the order which the updates for each factor gets applied, use the following code.\n\nX, stats, kwargs = factorize(Y; recursive_random_order=true, random_order=false, group_by_factor=true)\n\nA possible order of updates could now be the following. Note the order of factors is preserved (1, 2, 3) but the inner BlockedUpdates have a random order.\n\nBlockedUpdate(\n    BlockedUpdate(\n        Projection(1, Entrywise(ReLU, isnonnegative))\n        MomentumUpdate(1, lipschitz)\n        GradientStep(1, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        MomentumUpdate(2, lipschitz)\n        Projection(2, Entrywise(ReLU, isnonnegative))\n        GradientStep(2, gradient, LipschitzStep)\n    )\n    BlockedUpdate(\n        GradientStep(3, gradient, LipschitzStep)\n        MomentumUpdate(3, lipschitz)\n        Projection(3, Entrywise(ReLU, isnonnegative))\n    )\n)\n\nNote all the previously mentioned options still keeps the various updates for each factor together. For full randomization, use the following code.\n\nX, stats, kwargs = factorize(Y;\n    recursive_random_order=true, group_by_factor=false)\n\nA possible order of updates could now be the following. Note that every update can appear anywhere in the order.\n\nBlockedUpdate(\n    Projection(3, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(2, lipschitz)\n    GradientStep(2, gradient, LipschitzStep)\n    MomentumUpdate(1, lipschitz)\n    GradientStep(1, gradient, LipschitzStep)\n    Projection(2, Entrywise(ReLU, isnonnegative))\n    MomentumUpdate(3, lipschitz)\n    MomentumUpdate(2, lipschitz)\n    Projection(1, Entrywise(ReLU, isnonnegative))\n    GradientStep(3, gradient, LipschitzStep)\n)\n\nThe complete behaviour is summarized in the table below.\n\ngroup_by_factor random_order recursive_random_order Description\nfalse false false In the order given\nfalse false true In order given, but randomize how existing blocks are ordered (recursively)\nfalse true false Randomize updates, but keep existing blocks in order\nfalse true true Fully random\ntrue false false In the order given\ntrue false true In order of factors, but updates for each factor a random order\ntrue true false Random order of factors, preserve order of updates within each factor\ntrue true true Almost fully random, but updates for each factor are done together","category":"section"}]
}
